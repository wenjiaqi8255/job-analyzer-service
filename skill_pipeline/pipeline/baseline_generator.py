import logging
from datetime import datetime
from typing import Dict, List

import numpy as np
from sentence_transformers import SentenceTransformer

from .config import PipelineConfig
from .schemas import BaselineVectorSet

logger = logging.getLogger(__name__)


class BaselineGenerator:
    """è¯­ä¹‰åŸºçº¿æ„å»ºå™¨"""

    def __init__(self, config: PipelineConfig, classification_model: SentenceTransformer):
        self.config = config
        self.classification_model = classification_model  # ç”¨äºåˆ†ç±»çš„æ¨¡å‹

        # åˆå§‹åŒ–ä¸“é—¨ç”¨äºå‘é‡ç”Ÿæˆçš„æ¨¡å‹
        logger.info("ğŸ”§ åˆå§‹åŒ–å‘é‡ç”Ÿæˆæ¨¡å‹...")
        try:
            self.vector_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("âœ… æˆåŠŸåŠ è½½å‘é‡ç”Ÿæˆæ¨¡å‹: all-MiniLM-L6-v2")
        except Exception as e:
            logger.error(f"âŒ æ— æ³•åŠ è½½å‘é‡ç”Ÿæˆæ¨¡å‹: {e}")
            raise

    def generate_all_baselines(self, role_classifications: Dict, industry_classifications: Dict) -> Dict:
        """ç”Ÿæˆæ‰€æœ‰åŸºçº¿ï¼ˆåˆ†ç¦»è¯­ä¹‰æ•°æ®å’Œå‘é‡æ•°æ®ï¼‰"""
        logger.info("ğŸ—ï¸ å¼€å§‹ç”Ÿæˆè¯­ä¹‰åŸºçº¿...")

        baselines = {
            'global': self._generate_global_baseline(role_classifications, industry_classifications),
            'roles': self._generate_role_baselines(role_classifications),
            'industries': self._generate_industry_baselines(industry_classifications)
        }

        logger.info("âœ… æ‰€æœ‰åŸºçº¿ç”Ÿæˆå®Œæˆ")
        return baselines

    def _generate_global_baseline(self, role_classifications: Dict, industry_classifications: Dict) -> Dict:
        """ç”Ÿæˆå…¨å±€åŸºçº¿"""
        logger.info("ğŸŒ ç”Ÿæˆå…¨å±€åŸºçº¿...")

        all_skills_with_weights = []
        for categories in role_classifications.values():
            for skills in categories.values():
                all_skills_with_weights.extend(skills)

        for categories in industry_classifications.values():
            for skills in categories.values():
                all_skills_with_weights.extend(skills)

        # æŒ‰æŠ€èƒ½åç§°å»é‡ï¼Œä¿ç•™æœ€é«˜æƒé‡
        skill_map = {}
        for s in all_skills_with_weights:
            if s['skill'] not in skill_map or s['weight'] > skill_map[s['skill']]['weight']:
                skill_map[s['skill']] = s
        
        unique_skills_with_weights = sorted(list(skill_map.values()), key=lambda x: x['skill'])
        all_skills = [s['skill'] for s in unique_skills_with_weights]

        # ç»Ÿè®¡ä¿¡æ¯
        role_distribution = {role: sum(len(skills) for skills in categories.values())
                             for role, categories in role_classifications.items()}
        industry_distribution = {industry: sum(len(skills) for skills in categories.values())
                                 for industry, categories in industry_classifications.items()}

        # åˆ†ç¦»è¯­ä¹‰æ•°æ®å’Œå‘é‡æ•°æ®
        semantic_data = {
            'total_skills': len(all_skills),
            'skills_list': unique_skills_with_weights,  # æŠ€èƒ½åˆ—è¡¨ï¼ˆåŒ…å«æƒé‡ï¼‰
            'statistics': {
                'role_distribution': role_distribution,
                'industry_distribution': industry_distribution,
                'total_roles': len(role_classifications),
                'total_industries': len(industry_classifications)
            },
            'metadata': {
                'generation_date': datetime.now().isoformat(),
                'classification_model': self.config.classification_model,
                'vector_model': 'all-MiniLM-L6-v2',
                'embedding_dimension': 384  # all-MiniLM-L6-v2 çš„ç»´åº¦
            }
        }

        # ç”Ÿæˆå‘é‡æ•°æ®
        vector_data = self._generate_vectors_for_skills(unique_skills_with_weights)

        return {
            'semantic_data': semantic_data,
            'vector_data': vector_data
        }

    def _generate_role_baselines(self, role_classifications: Dict) -> Dict:
        """ç”Ÿæˆè§’è‰²åŸºçº¿"""
        logger.info("ğŸ‘” ç”Ÿæˆè§’è‰²åŸºçº¿...")

        role_baselines = {}

        for role, categories in role_classifications.items():
            all_role_skills_with_weights = []
            for skills in categories.values():
                all_role_skills_with_weights.extend(skills)
            
            # æŒ‰æŠ€èƒ½åå»é‡ï¼Œä¿ç•™æœ€é«˜æƒé‡ (å› ä¸ºä¸€ä¸ªæŠ€èƒ½å¯èƒ½é€šè¿‡æ˜¾å¼å’Œè¯­ä¹‰ä¸¤ç§æ–¹å¼åˆ†é…åˆ°åŒä¸€ä¸ªè§’è‰²)
            skill_map = {}
            for s in all_role_skills_with_weights:
                if s['skill'] not in skill_map or s['weight'] > skill_map[s['skill']]['weight']:
                    skill_map[s['skill']] = s
            
            unique_skills_with_weights = sorted(list(skill_map.values()), key=lambda x: x['weight'], reverse=True)

            if len(unique_skills_with_weights) < self.config.min_skills_for_baseline:
                logger.warning(f"è§’è‰² {role} æŠ€èƒ½æ•°é‡ä¸è¶³ ({len(unique_skills_with_weights)}), è·³è¿‡åŸºçº¿ç”Ÿæˆ")
                continue

            # è¯­ä¹‰æ•°æ®ï¼ˆäººç±»å¯è¯»ï¼‰
            semantic_data = {
                'role_name': role,
                'categories': categories, # categories already contain weights
                'total_skills': len(unique_skills_with_weights),
                'skills_list': unique_skills_with_weights,
                'statistics': {
                    'category_distribution': {cat: len(skills) for cat, skills in categories.items() if skills}
                },
                'metadata': {
                    'generation_date': datetime.now().isoformat(),
                    'classification_model': self.config.classification_model,
                    'vector_model': 'all-MiniLM-L6-v2'
                }
            }

            # å‘é‡æ•°æ®
            vector_data = self._generate_vectors_for_skills(unique_skills_with_weights)

            role_baselines[role] = {
                'semantic_data': semantic_data,
                'vector_data': vector_data
            }

        return role_baselines

    def _generate_industry_baselines(self, industry_classifications: Dict) -> Dict:
        """ç”Ÿæˆè¡Œä¸šåŸºçº¿"""
        logger.info("ğŸ­ ç”Ÿæˆè¡Œä¸šåŸºçº¿...")

        industry_baselines = {}

        for industry, categories in industry_classifications.items():
            all_industry_skills_with_weights = []
            for skills in categories.values():
                all_industry_skills_with_weights.extend(skills)

            # æŒ‰æŠ€èƒ½åå»é‡
            skill_map = {}
            for s in all_industry_skills_with_weights:
                if s['skill'] not in skill_map or s['weight'] > skill_map[s['skill']]['weight']:
                    skill_map[s['skill']] = s
            unique_skills_with_weights = sorted(list(skill_map.values()), key=lambda x: x['weight'], reverse=True)


            if len(unique_skills_with_weights) < self.config.min_skills_for_baseline:
                logger.warning(f"è¡Œä¸š {industry} æŠ€èƒ½æ•°é‡ä¸è¶³ ({len(unique_skills_with_weights)}), è·³è¿‡åŸºçº¿ç”Ÿæˆ")
                continue

            # è¯­ä¹‰æ•°æ®ï¼ˆäººç±»å¯è¯»ï¼‰
            semantic_data = {
                'industry_name': industry,
                'categories': categories,
                'total_skills': len(unique_skills_with_weights),
                'skills_list': unique_skills_with_weights,
                'statistics': {
                    'category_distribution': {cat: len(skills) for cat, skills in categories.items() if skills}
                },
                'metadata': {
                    'generation_date': datetime.now().isoformat(),
                    'classification_model': self.config.classification_model,
                    'vector_model': 'all-MiniLM-L6-v2'
                }
            }

            # å‘é‡æ•°æ®
            vector_data = self._generate_vectors_for_skills(unique_skills_with_weights)

            industry_baselines[industry] = {
                'semantic_data': semantic_data,
                'vector_data': vector_data
            }

        return industry_baselines

    def _generate_vectors_for_skills(self, skills_with_weights: List[Dict]) -> Dict:
        """
        ä¸ºæŠ€èƒ½åˆ—è¡¨ç”Ÿæˆç¬¦åˆBaselineVectorSet schemaçš„å‘é‡æ•°æ®å­—å…¸ã€‚
        ç°åœ¨æ¥æ”¶ä¸€ä¸ªåŒ…å«æŠ€èƒ½å’Œæƒé‡çš„å­—å…¸åˆ—è¡¨ï¼Œå¹¶è®¡ç®—åŠ æƒå¹³å‡å‘é‡ã€‚
        """
        if not skills_with_weights:
            return {}

        skills = [item['skill'] for item in skills_with_weights]
        weights = np.array([item['weight'] for item in skills_with_weights])

        try:
            skill_embeddings = self.vector_model.encode(skills)
            
            # è®¡ç®—åŠ æƒå¹³å‡å‘é‡
            weighted_average_vector = np.average(skill_embeddings, axis=0, weights=weights)

            # åˆ›å»ºç¬¦åˆBaselineVectorSet schemaçš„å­—å…¸
            # æ³¨æ„ï¼š'vectors' å­—æ®µç°åœ¨å­˜å‚¨çš„æ˜¯å•ä¸ªåŠ æƒå¹³å‡å‘é‡
            vector_set = BaselineVectorSet(
                vectors=[weighted_average_vector.tolist()],  # è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«ä¸€ä¸ªå‘é‡
                skill_map={skill: i for i, skill in enumerate(skills)}, # skill_map ä¿æŒä¸å˜
                model='all-MiniLM-L6-v2_weighted_avg', # æ›´æ–°æ¨¡å‹åç§°ä»¥åæ˜ å˜åŒ–
                dimension=384,
                # å¯ä»¥é€‰æ‹©æ€§åœ°æ·»åŠ æƒé‡ä¿¡æ¯
                weights={item['skill']: item['weight'] for item in skills_with_weights}
            )
            
            # Pydanticæ¨¡å‹è½¬ä¸ºå­—å…¸ä»¥ä¾¿JSONåºåˆ—åŒ–
            return vector_set.dict()
            
        except Exception as e:
            logger.error(f"å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return {}

    def _generate_vectors_for_role(self, categories: Dict) -> Dict:
        """DEPRECATED: å‘é‡ç”Ÿæˆå·²ç»Ÿä¸€åˆ° _generate_vectors_for_skills"""
        pass

    def _generate_vectors_for_industry(self, categories: Dict) -> Dict:
        """DEPRECATED: å‘é‡ç”Ÿæˆå·²ç»Ÿä¸€åˆ° _generate_vectors_for_skills"""
        pass 
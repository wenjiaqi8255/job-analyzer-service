import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List

from .config import PipelineConfig
from .database_manager import DatabaseManager
from .skill_processor import SkillProcessor
from .semantic_classifier import SemanticClassifier
from .baseline_generator import BaselineGenerator

logger = logging.getLogger(__name__)


class PipelineOrchestrator:
    """æ•´ä½“æµç¨‹ç¼–æ’å™¨"""

    def __init__(self, config: PipelineConfig):
        self.config = config
        self.processor = SkillProcessor()
        config_dir = config.semantic_config_dir  # æ–°å¢é…ç½®ç›®å½•è·¯å¾„
        self.classifier = SemanticClassifier(config_dir)
        # ä¿®æ”¹åˆå§‹åŒ–åŸºçº¿ç”Ÿæˆå™¨çš„è°ƒç”¨
        self.baseline_generator = BaselineGenerator(config, self.classifier.semantic_engine.model)
        self.database_manager = DatabaseManager(config)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.config.output_dir).mkdir(exist_ok=True)

    def run_complete_pipeline(self, vocab_path: str) -> Dict:
        """è¿è¡Œå®Œæ•´ç®¡çº¿"""
        logger.info("ğŸš€ å¯åŠ¨ç«¯åˆ°ç«¯æŠ€èƒ½å¤„ç†ç®¡çº¿")
        logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {vocab_path}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.config.output_dir}")
        print("-" * 60)

        pipeline_start = datetime.now()
        results = {}

        try:
            # 1. åŠ è½½å’Œå¤„ç†è¯æ±‡è¡¨
            logger.info("ğŸ“Š Step 1: åŠ è½½æŠ€èƒ½è¯æ±‡è¡¨...")
            skills_vocab = self._load_vocabulary(vocab_path)
            all_skills = self.processor.extract_all_skills(skills_vocab)
            logger.info(f"ğŸ“‹ æå–åˆ° {len(all_skills)} ä¸ªæŠ€èƒ½")
            results['skills_extracted'] = len(all_skills)

            # 2. è¯­ä¹‰åˆ†ç±»
            logger.info("ğŸ§  Step 2: æ‰§è¡Œè¯­ä¹‰åˆ†ç±»...")
            role_classifications, industry_classifications = self.classifier.classify_skills(all_skills)
            results['role_classifications'] = role_classifications
            results['industry_classifications'] = industry_classifications

            # ä¿å­˜åˆ†ç±»ç»“æœ
            self._save_classifications(role_classifications, industry_classifications)

            # 3. ç”Ÿæˆè¯­ä¹‰åŸºçº¿
            logger.info("ğŸ—ï¸ Step 3: ç”Ÿæˆè¯­ä¹‰åŸºçº¿...")
            baselines = self.baseline_generator.generate_all_baselines(
                role_classifications, industry_classifications
            )
            results['baselines'] = baselines

            # ä¿å­˜åŸºçº¿æ–‡ä»¶
            self._save_baselines(baselines)

            # 4. ä¸Šä¼ åˆ°æ•°æ®åº“
            logger.info("ğŸ“¤ Step 4: ä¸Šä¼ åˆ°æ•°æ®åº“...")
            upload_results = self.database_manager.upload_baselines(baselines)
            results['upload_results'] = upload_results

            # 5. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            pipeline_end = datetime.now()
            duration = (pipeline_end - pipeline_start).total_seconds()

            results['summary'] = self._generate_summary_report(
                all_skills, role_classifications, industry_classifications,
                baselines, upload_results, duration
            )

            # ä¿å­˜æ€»ç»“æŠ¥å‘Š
            self._save_summary_report(results['summary'])

            logger.info("ğŸ‰ ç®¡çº¿æ‰§è¡Œå®Œæˆ!")
            self._print_summary(results['summary'])

        except Exception as e:
            logger.error(f"âŒ ç®¡çº¿æ‰§è¡Œå¤±è´¥: {e}")
            results['error'] = str(e)
            raise

        return results

    def _load_vocabulary(self, vocab_path: str) -> Dict:
        """åŠ è½½æŠ€èƒ½è¯æ±‡è¡¨"""
        vocab_path = Path(vocab_path)
        if not vocab_path.exists():
            raise FileNotFoundError(f"è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}")

        with open(vocab_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # å°è¯•ç›´æ¥è§£æJSON
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä»markdownä¸­æå–JSON
            import re
            json_match = re.search(r'\{[\s\S]*?\}(?=\s*###|\s*$)', content)
            if json_match:
                return json.loads(json_match.group())
            else:
                raise ValueError("æ— æ³•è§£ææŠ€èƒ½è¯æ±‡è¡¨æ–‡ä»¶")

    def _save_classifications(self, role_classifications: Dict, industry_classifications: Dict):
        """ä¿å­˜åˆ†ç±»ç»“æœ"""
        # ä¿å­˜è§’è‰²åˆ†ç±»
        role_path = Path(self.config.output_dir) / "role_skill_classifications.json"
        with open(role_path, 'w', encoding='utf-8') as f:
            json.dump(role_classifications, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è§’è‰²åˆ†ç±»å·²ä¿å­˜: {role_path}")

        # ä¿å­˜è¡Œä¸šåˆ†ç±»
        industry_path = Path(self.config.output_dir) / "industry_skill_classifications.json"
        with open(industry_path, 'w', encoding='utf-8') as f:
            json.dump(industry_classifications, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è¡Œä¸šåˆ†ç±»å·²ä¿å­˜: {industry_path}")

        # ä¿å­˜åˆ†ç±»è¯¦æƒ…ï¼ˆå¦‚æœå¯ç”¨è°ƒè¯•ï¼‰
        if self.config.save_classification_details:
            self._save_classification_details(role_classifications, industry_classifications)

    def _save_classification_details(self, role_classifications: Dict, industry_classifications: Dict):
        """ä¿å­˜åˆ†ç±»è¯¦æƒ…ç»Ÿè®¡"""
        details = {
            'classification_summary': {
                'total_roles_with_skills': len(
                    [r for r in role_classifications.values() if any(skills for skills in r.values())]),
                'total_industries_with_skills': len(
                    [i for i in industry_classifications.values() if any(skills for skills in i.values())]),
                'role_skill_distribution': {
                    role: {cat: len(skills) for cat, skills in categories.items() if skills}
                    for role, categories in role_classifications.items()
                    if any(skills for skills in categories.values())
                },
                'industry_skill_distribution': {
                    industry: {cat: len(skills) for cat, skills in categories.items() if skills}
                    for industry, categories in industry_classifications.items()
                    if any(skills for skills in categories.values())
                }
            },
            'quality_insights': {
                'largest_role_categories': self._get_largest_categories(role_classifications),
                'largest_industry_categories': self._get_largest_categories(industry_classifications),
                'potential_issues': self._identify_potential_issues(role_classifications, industry_classifications)
            },
            'configuration_used': {
                'high_confidence_threshold': self.config.high_confidence_threshold,
                'medium_confidence_threshold': self.config.medium_confidence_threshold,
                'low_confidence_threshold': self.config.low_confidence_threshold,
                'model_name': self.config.classification_model
            }
        }

        details_path = Path(self.config.output_dir) / "classification_details.json"
        with open(details_path, 'w', encoding='utf-8') as f:
            json.dump(details, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“‹ åˆ†ç±»è¯¦æƒ…å·²ä¿å­˜: {details_path}")

    def _get_largest_categories(self, classifications: Dict) -> List[Dict]:
        """è·å–æœ€å¤§çš„åˆ†ç±»ç±»åˆ«"""
        largest = []
        for name, categories in classifications.items():
            for category, skills in categories.items():
                if skills:
                    largest.append({
                        'name': name,
                        'category': category,
                        'skill_count': len(skills),
                        'sample_skills': skills[:5]  # åªä¿å­˜å‰5ä¸ªæŠ€èƒ½ä½œä¸ºç¤ºä¾‹
                    })

        # æŒ‰æŠ€èƒ½æ•°é‡æ’åº
        largest.sort(key=lambda x: x['skill_count'], reverse=True)
        return largest[:10]  # è¿”å›å‰10ä¸ªæœ€å¤§çš„ç±»åˆ«

    def _identify_potential_issues(self, role_classifications: Dict, industry_classifications: Dict) -> List[str]:
        """è¯†åˆ«æ½œåœ¨çš„åˆ†ç±»é—®é¢˜"""
        issues = []

        # æ£€æŸ¥æ˜¯å¦æœ‰è§’è‰²å ç”¨è¿‡å¤šæŠ€èƒ½
        total_role_skills = sum(sum(len(skills) for skills in categories.values())
                                for categories in role_classifications.values())

        for role, categories in role_classifications.items():
            role_skills = sum(len(skills) for skills in categories.values())
            if total_role_skills > 0 and role_skills > total_role_skills * 0.3:  # å¦‚æœå•ä¸ªè§’è‰²å ç”¨è¶…è¿‡30%çš„æŠ€èƒ½
                issues.append(
                    f"è§’è‰² '{role}' å¯èƒ½å ç”¨äº†è¿‡å¤šæŠ€èƒ½ ({role_skills} ä¸ª, {role_skills / total_role_skills * 100:.1f}%)")

        # æ£€æŸ¥æ˜¯å¦æœ‰è¡Œä¸šä¸ºç©º
        empty_industries = [industry for industry, categories in industry_classifications.items()
                            if not any(skills for skills in categories.values())]
        if len(empty_industries) > len(industry_classifications) * 0.5:
            issues.append(f"è¶…è¿‡ä¸€åŠçš„è¡Œä¸šåˆ†ç±»ä¸ºç©º ({len(empty_industries)}/{len(industry_classifications)})")

        # æ£€æŸ¥ç‰¹å®šæŠ€èƒ½æ˜¯å¦è¢«æ­£ç¡®åˆ†ç±»
        design_tools = ['figma', 'sketch', 'adobe_xd']
        ui_ux_skills = []
        if 'ui_ux_designer' in role_classifications:
            ui_ux_skills = [skill for category in role_classifications['ui_ux_designer'].values()
                            for skill in category]

        missing_design_tools = [tool for tool in design_tools if tool not in ui_ux_skills]
        if missing_design_tools:
            issues.append(f"è®¾è®¡å·¥å…·å¯èƒ½æœªæ­£ç¡®åˆ†ç±»åˆ°UI/UXè®¾è®¡å¸ˆ: {missing_design_tools}")

        return issues

    def _save_baselines(self, baselines: Dict):
        """ä¿å­˜åŸºçº¿æ–‡ä»¶ï¼ˆåˆ†ç¦»è¯­ä¹‰å’Œå‘é‡æ•°æ®ï¼‰"""
        # ä¿å­˜å…¨å±€åŸºçº¿ - è¯­ä¹‰æ•°æ®
        global_semantic_path = Path(self.config.output_dir) / "global_baseline_semantic.json"
        with open(global_semantic_path, 'w', encoding='utf-8') as f:
            json.dump(baselines['global']['semantic_data'], f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ å…¨å±€è¯­ä¹‰åŸºçº¿å·²ä¿å­˜: {global_semantic_path}")

        # ä¿å­˜å…¨å±€åŸºçº¿ - å‘é‡æ•°æ®
        global_vector_path = Path(self.config.output_dir) / "global_baseline_vectors.json"
        with open(global_vector_path, 'w', encoding='utf-8') as f:
            json.dump(baselines['global']['vector_data'], f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ å…¨å±€å‘é‡åŸºçº¿å·²ä¿å­˜: {global_vector_path}")

        # ä¿å­˜è§’è‰²åŸºçº¿ - è¯­ä¹‰æ•°æ®
        role_semantic_data = {role: data['semantic_data'] for role, data in baselines['roles'].items()}
        role_semantic_path = Path(self.config.output_dir) / "role_baselines_semantic.json"
        with open(role_semantic_path, 'w', encoding='utf-8') as f:
            json.dump(role_semantic_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è§’è‰²è¯­ä¹‰åŸºçº¿å·²ä¿å­˜: {role_semantic_path}")

        # ä¿å­˜è§’è‰²åŸºçº¿ - å‘é‡æ•°æ®
        role_vector_data = {role: data['vector_data'] for role, data in baselines['roles'].items()}
        role_vector_path = Path(self.config.output_dir) / "role_baselines_vectors.json"
        with open(role_vector_path, 'w', encoding='utf-8') as f:
            json.dump(role_vector_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è§’è‰²å‘é‡åŸºçº¿å·²ä¿å­˜: {role_vector_path}")

        # ä¿å­˜è¡Œä¸šåŸºçº¿ - è¯­ä¹‰æ•°æ®
        industry_semantic_data = {industry: data['semantic_data'] for industry, data in
                                  baselines['industries'].items()}
        industry_semantic_path = Path(self.config.output_dir) / "industry_baselines_semantic.json"
        with open(industry_semantic_path, 'w', encoding='utf-8') as f:
            json.dump(industry_semantic_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è¡Œä¸šè¯­ä¹‰åŸºçº¿å·²ä¿å­˜: {industry_semantic_path}")

        # ä¿å­˜è¡Œä¸šåŸºçº¿ - å‘é‡æ•°æ®
        industry_vector_data = {industry: data['vector_data'] for industry, data in baselines['industries'].items()}
        industry_vector_path = Path(self.config.output_dir) / "industry_baselines_vectors.json"
        with open(industry_vector_path, 'w', encoding='utf-8') as f:
            json.dump(industry_vector_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è¡Œä¸šå‘é‡åŸºçº¿å·²ä¿å­˜: {industry_vector_path}")

    def _count_semantic_uploads(self, upload_results: Dict) -> int:
        """ç»Ÿè®¡è¯­ä¹‰æ•°æ®ä¸Šä¼ æ•°é‡"""
        count = 0
        if upload_results.get('global', {}).get('semantic_upload', {}).get('status') in ['uploaded', 'exists']:
            count += 1
        for result in upload_results.get('roles', {}).values():
            if result.get('semantic_upload', {}).get('status') in ['uploaded', 'exists']:
                count += 1
        for result in upload_results.get('industries', {}).values():
            if result.get('semantic_upload', {}).get('status') in ['uploaded', 'exists']:
                count += 1
        return count

    def _count_vector_uploads(self, upload_results: Dict) -> int:
        """ç»Ÿè®¡æˆåŠŸä¸Šä¼ çš„å‘é‡æ•°é‡"""
        count = 0
        if 'global' in upload_results and upload_results['global'].get('vector_data', {}).get('success'):
            count += 1
        if 'roles' in upload_results:
            count += sum(1 for res in upload_results['roles'].values() if res.get('vector_data', {}).get('success'))
        if 'industries' in upload_results:
            count += sum(
                1 for res in upload_results['industries'].values() if res.get('vector_data', {}).get('success'))
        return count

    def _generate_summary_report(self, all_skills: List[Dict], role_classifications: Dict,
                                industry_classifications: Dict, baselines: Dict,
                                upload_results: Dict, duration: float) -> Dict:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        # å¢åŠ å¥å£®æ€§ï¼Œå¤„ç† upload_results å¯èƒ½ä¸æ˜¯å­—å…¸çš„æƒ…å†µ
        if not isinstance(upload_results, dict):
            logger.warning(f"âš ï¸ 'upload_results' ä¸æ˜¯ä¸€ä¸ªå­—å…¸, è€Œæ˜¯ {type(upload_results)}. "
                           f"å°†ä½¿ç”¨ç©ºçš„ä¸Šä¼ ç»Ÿè®¡æ•°æ®. æ”¶åˆ°çš„å€¼: {upload_results}")
            upload_results = {}
            
        total_skills = len(all_skills)
        total_roles = len(role_classifications)
        total_industries = len(industry_classifications)

        # ç»Ÿè®¡åˆ†ç±»ç»“æœ
        total_role_skills = sum(sum(len(skills) for skills in categories.values())
                                for categories in role_classifications.values())
        total_industry_skills = sum(sum(len(skills) for skills in categories.values())
                                    for categories in industry_classifications.values())

        # ç»Ÿè®¡ä¸Šä¼ ç»“æœ - æ›´æ–°ä»¥æ”¯æŒæ–°çš„åˆ†ç¦»ä¸Šä¼ æ ¼å¼
        upload_stats = {
            'global_uploaded': upload_results.get('global', {}).get('status') == 'uploaded',
            'roles_uploaded': sum(1 for result in upload_results.get('roles', {}).values()
                                if result.get('status') == 'uploaded'),
            'industries_uploaded': sum(1 for result in upload_results.get('industries', {}).values()
                                       if result.get('status') == 'uploaded'),
            'total_semantic_uploads': self._count_semantic_uploads(upload_results),
            'total_vector_uploads': self._count_vector_uploads(upload_results),
            'total_errors': sum(1 for section in upload_results.values()
                              for result in (section.values() if isinstance(section, dict) else [section])
                              if result.get('status') == 'error')
        }

        return {
            'pipeline_info': {
                'execution_time_seconds': round(duration, 2),
                'completion_time': datetime.now().isoformat(),
                'config': {
                    'model_name': self.config.classification_model,
                    'role_threshold': self.config.role_threshold,
                    'industry_threshold': self.config.industry_threshold,
                    'min_skills_for_baseline': self.config.min_skills_for_baseline
                }
            },
            'skill_processing': {
                'total_skills_extracted': len(all_skills),
                'total_role_assignments': total_role_skills,
                'total_industry_assignments': total_industry_skills,
                'roles_with_skills': len(role_classifications),
                'industries_with_skills': len(industry_classifications)
            },
            'baseline_generation': {
                'global_baseline_generated': 'global' in baselines,
                'role_baselines_generated': len(baselines.get('roles', {})),
                'industry_baselines_generated': len(baselines.get('industries', {})),
                'total_baselines': 1 + len(baselines.get('roles', {})) + len(baselines.get('industries', {}))
            },
            'database_upload': upload_stats,
            'quality_metrics': {
                'classification_coverage': {
                    'role_coverage_percent': round(
                        (total_role_skills / len(all_skills)) * 100, 2) if all_skills else 0,
                    'industry_coverage_percent': round(
                        (total_industry_skills / len(all_skills)) * 100, 2) if all_skills else 0
                },
                'baseline_quality': {
                    'avg_skills_per_role': round(total_role_skills / len(
                        role_classifications), 2) if role_classifications else 0,
                    'avg_skills_per_industry': round(total_industry_skills / len(
                        industry_classifications), 2) if industry_classifications else 0
                }
            },
            'output_files': {
                'role_classifications': f"{self.config.output_dir}/role_skill_classifications.json",
                'industry_classifications': f"{self.config.output_dir}/industry_skill_classifications.json",
                # æ–°çš„åˆ†ç¦»æ ¼å¼æ–‡ä»¶
                'global_baseline_semantic': f"{self.config.output_dir}/global_baseline_semantic.json",
                'global_baseline_vectors': f"{self.config.output_dir}/global_baseline_vectors.json",
                'role_baselines_semantic': f"{self.config.output_dir}/role_baselines_semantic.json",
                'role_baselines_vectors': f"{self.config.output_dir}/role_baselines_vectors.json",
                'industry_baselines_semantic': f"{self.config.output_dir}/industry_baselines_semantic.json",
                'industry_baselines_vectors': f"{self.config.output_dir}/industry_baselines_vectors.json",
                'summary_report': f"{self.config.output_dir}/pipeline_summary.json"
            }
        }

    def _save_summary_report(self, summary: Dict):
        """ä¿å­˜æ€»ç»“æŠ¥å‘Š"""
        summary_path = Path(self.config.output_dir) / "pipeline_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")

    def _print_summary(self, summary: Dict):
        """æ‰“å°æ€»ç»“ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ç®¡çº¿æ‰§è¡Œæ€»ç»“")
        print("=" * 60)

        # åŸºæœ¬ä¿¡æ¯
        pipeline_info = summary['pipeline_info']
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {pipeline_info['execution_time_seconds']} ç§’")
        print(f"ğŸ• å®Œæˆæ—¶é—´: {pipeline_info['completion_time']}")

        # æŠ€èƒ½å¤„ç†
        skill_info = summary['skill_processing']
        print(f"\nğŸ“‹ æŠ€èƒ½å¤„ç†:")
        print(f"   æ€»æŠ€èƒ½æ•°: {skill_info['total_skills_extracted']}")
        print(f"   è§’è‰²åˆ†é…: {skill_info['total_role_assignments']}")
        print(f"   è¡Œä¸šåˆ†é…: {skill_info['total_industry_assignments']}")
        print(f"   æœ‰æ•ˆè§’è‰²: {skill_info['roles_with_skills']}")
        print(f"   æœ‰æ•ˆè¡Œä¸š: {skill_info['industries_with_skills']}")

        # åŸºçº¿ç”Ÿæˆ
        baseline_info = summary['baseline_generation']
        print(f"\nğŸ—ï¸ åŸºçº¿ç”Ÿæˆ:")
        print(f"   æ€»åŸºçº¿æ•°: {baseline_info['total_baselines']}")
        print(f"   è§’è‰²åŸºçº¿: {baseline_info['role_baselines_generated']}")
        print(f"   è¡Œä¸šåŸºçº¿: {baseline_info['industry_baselines_generated']}")

        # æ•°æ®åº“ä¸Šä¼ 
        upload_info = summary['database_upload']
        print(f"\nğŸ“¤ æ•°æ®åº“ä¸Šä¼ :")
        print(f"   å…¨å±€åŸºçº¿: {'âœ…' if upload_info['global_uploaded'] else 'âŒ'}")
        print(f"   è§’è‰²åŸºçº¿: {upload_info['roles_uploaded']} ä¸ª")
        print(f"   è¡Œä¸šåŸºçº¿: {upload_info['industries_uploaded']} ä¸ª")
        print(f"   è¯­ä¹‰æ•°æ®: {upload_info.get('total_semantic_uploads', 0)} ä¸ª")
        print(f"   å‘é‡æ•°æ®: {upload_info.get('total_vector_uploads', 0)} ä¸ª")
        print(f"   é”™è¯¯æ•°é‡: {upload_info['total_errors']}")

        # è´¨é‡æŒ‡æ ‡
        quality_info = summary['quality_metrics']
        print(f"\nğŸ“ˆ è´¨é‡æŒ‡æ ‡:")
        print(f"   è§’è‰²è¦†ç›–ç‡: {quality_info['classification_coverage']['role_coverage_percent']}%")
        print(f"   è¡Œä¸šè¦†ç›–ç‡: {quality_info['classification_coverage']['industry_coverage_percent']}%")
        print(f"   å¹³å‡è§’è‰²æŠ€èƒ½: {quality_info['baseline_quality']['avg_skills_per_role']}")
        print(f"   å¹³å‡è¡Œä¸šæŠ€èƒ½: {quality_info['baseline_quality']['avg_skills_per_industry']}")

        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.config.output_dir}")
        print("=" * 60)

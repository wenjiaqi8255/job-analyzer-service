#!/usr/bin/env python3
"""
JobbAI ç«¯åˆ°ç«¯æŠ€èƒ½å¤„ç†ç®¡çº¿
ä»vocabularyåˆ°ä¸‰ä¸ªbaselineçš„å®Œæ•´æµç¨‹
"""

import json
import hashlib
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
import logging
from dataclasses import dataclass
from collections import defaultdict, Counter

# ç¬¬ä¸‰æ–¹åº“
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from supabase import create_client, Client
from dotenv import load_dotenv
import os
import argparse

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class PipelineConfig:
    """ç®¡çº¿é…ç½®ç±»"""
    # æ¨¡å‹é…ç½®
    classification_model: str = 'MohammedDhiyaEddine/job-skill-sentence-transformer-tsdae'
    backup_model: str = 'all-MiniLM-L6-v2'
    
    # åˆ†çº§é˜ˆå€¼é…ç½® - æ›´ä¸¥æ ¼çš„é˜ˆå€¼è®¾ç½®
    high_confidence_threshold: float = 0.75  # é«˜ç½®ä¿¡åº¦é˜ˆå€¼
    medium_confidence_threshold: float = 0.55  # ä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼  
    low_confidence_threshold: float = 0.35  # ä½ç½®ä¿¡åº¦é˜ˆå€¼
    
    # å…¼å®¹æ—§æ¥å£çš„é˜ˆå€¼
    role_threshold: float = 0.55  # æé«˜é»˜è®¤è§’è‰²é˜ˆå€¼
    industry_threshold: float = 0.45  # æé«˜é»˜è®¤è¡Œä¸šé˜ˆå€¼
    
    # åŸºçº¿ç”Ÿæˆé…ç½®
    min_skills_for_baseline: int = 5
    embedding_dimension: int = 768
    
    # æ•°æ®åº“é…ç½®
    supabase_url: str = None
    supabase_key: str = None
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "output"
    backup_existing: bool = True
    
    # è°ƒè¯•é…ç½®
    enable_debug_logging: bool = False
    save_classification_details: bool = True
    
    def __post_init__(self):
        """åŠ è½½ç¯å¢ƒå˜é‡å’ŒéªŒè¯é…ç½®"""
        load_dotenv()
        if not self.supabase_url:
            self.supabase_url = os.getenv("SUPABASE_URL")
        if not self.supabase_key:
            self.supabase_key = os.getenv("SUPABASE_KEY")
            
        # éªŒè¯é˜ˆå€¼åˆç†æ€§
        if self.high_confidence_threshold <= self.medium_confidence_threshold:
            logger.warning("âš ï¸ é«˜ç½®ä¿¡åº¦é˜ˆå€¼åº”è¯¥å¤§äºä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼")
        if self.medium_confidence_threshold <= self.low_confidence_threshold:
            logger.warning("âš ï¸ ä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼åº”è¯¥å¤§äºä½ç½®ä¿¡åº¦é˜ˆå€¼")

class SkillProcessor:
    """æŠ€èƒ½é¢„å¤„ç†å’Œæ ‡å‡†åŒ–"""
    
    def __init__(self):
        self.skill_types = {
            'programming_language': ['python', 'java', 'javascript', 'typescript', 'c++', 'c#', 'ruby', 'go', 'rust', 'scala', 'kotlin', 'swift'],
            'framework': ['react', 'angular', 'vue', 'django', 'flask', 'spring', 'laravel', 'express'],
            'database': ['mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch', 'cassandra'],
            'cloud_service': ['aws', 'azure', 'google_cloud', 'docker', 'kubernetes'],
            'tool': ['figma', 'sketch', 'photoshop', 'tableau', 'jira', 'confluence'],
            'methodology': ['agile', 'scrum', 'lean', 'kanban', 'devops'],
            'certification': ['certified', 'certification', 'associate', 'professional'],
            'language': ['english', 'german', 'french', 'spanish', 'chinese', 'japanese']
        }
    
    def extract_all_skills(self, skills_vocab: Dict) -> List[Dict]:
        """ä»æŠ€èƒ½è¯æ±‡ä¸­æå–æ‰€æœ‰æŠ€èƒ½"""
        all_skills = []
        
        def extract_recursive(data, path="", context=""):
            if isinstance(data, dict):
                for key, value in data.items():
                    new_path = f"{path}.{key}" if path else key
                    new_context = f"{context} {key}".strip()
                    if isinstance(value, list):
                        for skill in value:
                            all_skills.append({
                                'skill': skill,
                                'path': new_path,
                                'context': new_context,
                                'skill_type': self._infer_skill_type(skill, key)
                            })
                    else:
                        extract_recursive(value, new_path, new_context)
        
        extract_recursive(skills_vocab)
        return all_skills
    
    def _infer_skill_type(self, skill: str, category: str) -> str:
        """æ¨æ–­æŠ€èƒ½ç±»å‹"""
        skill_lower = skill.lower()
        category_lower = category.lower()
        
        for skill_type, keywords in self.skill_types.items():
            if any(keyword in skill_lower for keyword in keywords) or skill_type in category_lower:
                return skill_type
        
        return 'domain_knowledge'

class SemanticClassifier:
    """åŸºäºæ˜¾å¼è§„åˆ™ + è¯­ä¹‰è¡¥å……çš„æ™ºèƒ½åˆ†ç±»å™¨"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.model = self._load_model()
        self._load_semantic_descriptions()
        self._load_explicit_mappings()
        
        # åˆ†çº§é˜ˆå€¼
        self.high_confidence_threshold = getattr(config, 'high_confidence_threshold', 0.75)
        self.medium_confidence_threshold = getattr(config, 'medium_confidence_threshold', 0.55)
        self.low_confidence_threshold = getattr(config, 'low_confidence_threshold', 0.35)
        
    def _load_model(self) -> SentenceTransformer:
        """åŠ è½½SentenceTransformeræ¨¡å‹"""
        logger.info("ğŸ¤– åŠ è½½SentenceTransformeræ¨¡å‹...")
        try:
            model = SentenceTransformer(self.config.classification_model)
            logger.info(f"âœ… æˆåŠŸåŠ è½½ä¸“ç”¨æ¨¡å‹: {self.config.classification_model}")
            return model
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•åŠ è½½ä¸“ç”¨æ¨¡å‹ï¼Œä½¿ç”¨å¤‡é€‰æ¨¡å‹: {e}")
            model = SentenceTransformer(self.config.backup_model)
            logger.info(f"âœ… æˆåŠŸåŠ è½½å¤‡é€‰æ¨¡å‹: {self.config.backup_model}")
            return model
    
    def _assign_explicit_role(self, skill: str, role_classifications: Dict) -> bool:
        """æ˜¾å¼è§’è‰²åˆ†é…"""
        skill_lower = skill.lower()
        
        for role, categories in self.explicit_role_mappings.items():
            for category, skills in categories.items():
                if skill in skills or skill_lower in skills:
                    role_classifications[role][category].append(skill)
                    logger.debug(f"  ğŸ“‹ æ˜¾å¼åˆ†é…: {skill} â†’ {role}.{category}")
                    return True
        return False
    
    def _assign_explicit_industry(self, skill: str, industry_classifications: Dict) -> bool:
        """æ˜¾å¼è¡Œä¸šåˆ†é…"""
        skill_lower = skill.lower()
        
        for industry, categories in self.explicit_industry_mappings.items():
            for category, skills in categories.items():
                if skill in skills or skill_lower in skills:
                    industry_classifications[industry][category].append(skill)
                    logger.debug(f"  ğŸ¢ æ˜¾å¼åˆ†é…: {skill} â†’ {industry}.{category}")
                    return True
        return False
    
    def _is_excluded_by_rules(self, skill_info: Dict) -> bool:
        """æ£€æŸ¥æŠ€èƒ½æ˜¯å¦è¢«æ’é™¤è§„åˆ™æ’é™¤"""
        skill = skill_info['skill']
        skill_type = skill_info['skill_type']
        
        for role, rules in self.exclusion_rules.items():
            exclude_skills = rules.get('exclude_skills', [])
            exclude_categories = rules.get('exclude_categories', [])
            
            if skill in exclude_skills or skill_type in exclude_categories:
                logger.debug(f"  âŒ æ’é™¤è§„åˆ™: {skill} ä¸é€‚åˆ {role}")
                return True
        
        return False
    
    def enhance_skill_description(self, skill_info: Dict) -> str:
        """å¢å¼ºæŠ€èƒ½æè¿°"""
        skill = skill_info['skill']
        skill_type = skill_info['skill_type']
        context = skill_info['context']
        
        base_description = skill.replace('_', ' ').replace('-', ' ')
        
        enhancement_templates = {
            'programming_language': f'{base_description} programming language software development coding',
            'framework': f'{base_description} framework web development software engineering',
            'database': f'{base_description} database management data storage query optimization',
            'cloud_service': f'{base_description} cloud computing infrastructure deployment scalability',
            'tool': f'{base_description} tool software productivity development',
            'methodology': f'{base_description} methodology approach process management',
            'certification': f'{base_description} certification professional qualification expertise',
            'language': f'{base_description} language communication international business',
            'domain_knowledge': f'{base_description} {context} domain expertise industry knowledge'
        }
        
        return enhancement_templates.get(skill_type, f"{base_description} {context}")
    
    def _load_explicit_mappings(self):
        """åŠ è½½æ˜¾å¼æŠ€èƒ½æ˜ å°„è§„åˆ™"""
        logger.info("ğŸ“‹ åŠ è½½æ˜¾å¼æŠ€èƒ½æ˜ å°„è§„åˆ™...")
        
        # æ ¸å¿ƒæ˜¾å¼æŠ€èƒ½æ˜ å°„ - ç¡®ä¿è¿™äº›æŠ€èƒ½åˆ†é…æ­£ç¡®
        self.explicit_role_mappings = {
            'ui_ux_designer': {
                'essential': [
                    'figma', 'sketch', 'adobe_xd', 'adobe_photoshop', 'adobe_illustrator',
                    'invision', 'principle', 'framer', 'zeplin', 'marvel',
                    'user_interface_design', 'user_experience_design', 'ui_design', 'ux_design',
                    'wireframing', 'prototyping', 'user_research', 'usability_testing',
                    'interaction_design', 'visual_design', 'design_systems'
                ],
                'common': ['adobe_creative_suite', 'canva', 'miro', 'figjam']
            },
            'digital_marketing_specialist': {
                'essential': [
                    'google_ads', 'facebook_ads', 'google_analytics', 'social_media_marketing',
                    'seo', 'sem', 'content_marketing', 'email_marketing', 'instagram_marketing',
                    'linkedin_marketing', 'twitter_marketing', 'youtube_marketing',
                    'google_tag_manager', 'facebook_pixel', 'mailchimp', 'hubspot'
                ],
                'common': ['adobe_creative_suite', 'canva', 'hootsuite', 'buffer']
            },
            'project_manager': {
                'essential': [
                    'agile', 'scrum', 'project_management', 'jira', 'confluence',
                    'kanban', 'waterfall', 'risk_management', 'stakeholder_management',
                    'budget_management', 'timeline_management', 'resource_planning'
                ],
                'common': ['microsoft_project', 'asana', 'trello', 'monday_com', 'slack']
            },
            'data_scientist': {
                'essential': [
                    'python', 'r', 'sql', 'pandas', 'numpy', 'scikit_learn', 'tensorflow',
                    'pytorch', 'jupyter', 'matplotlib', 'seaborn', 'plotly',
                    'machine_learning', 'deep_learning', 'statistical_analysis',
                    'data_visualization', 'data_mining', 'predictive_modeling'
                ],
                'common': ['tableau', 'power_bi', 'excel', 'spark', 'hadoop']
            },
            'cybersecurity_specialist': {
                'essential': [
                    'penetration_testing', 'vulnerability_assessment', 'incident_response',
                    'security_auditing', 'threat_modeling', 'malware_analysis',
                    'network_security', 'web_application_security', 'cryptography',
                    'security_frameworks', 'iso_27001', 'nist_framework'
                ],
                'common': ['kali_linux', 'metasploit', 'wireshark', 'burp_suite', 'nmap']
            },
            'frontend_web_developer': {
                'essential': [
                    'html', 'css', 'javascript', 'react', 'angular', 'vue',
                    'typescript', 'sass', 'less', 'webpack', 'npm', 'yarn',
                    'responsive_design', 'cross_browser_compatibility'
                ],
                'common': ['figma', 'sketch', 'git', 'vscode']
            },
            'backend_developer': {
                'essential': [
                    'python', 'java', 'nodejs', 'c#', 'php', 'ruby', 'go',
                    'api_development', 'rest_api', 'graphql', 'microservices',
                    'database_design', 'sql', 'mongodb', 'postgresql', 'mysql'
                ],
                'common': ['docker', 'kubernetes', 'redis', 'elasticsearch']
            },
            'devops_engineer': {
                'essential': [
                    'docker', 'kubernetes', 'jenkins', 'gitlab_ci', 'github_actions',
                    'terraform', 'ansible', 'chef', 'puppet', 'aws', 'azure', 'gcp',
                    'continuous_integration', 'continuous_deployment', 'infrastructure_as_code'
                ],
                'common': ['linux', 'bash', 'python', 'monitoring', 'logging']
            },
            'cloud_engineer': {
                'essential': [
                    'aws', 'azure', 'google_cloud', 'docker', 'kubernetes',
                    'terraform', 'cloudformation', 'serverless', 'lambda',
                    'cloud_architecture', 'cloud_security', 'cost_optimization'
                ],
                'common': ['python', 'bash', 'networking', 'monitoring']
            }
        }
        
        # æ˜¾å¼è¡Œä¸šæ˜ å°„
        self.explicit_industry_mappings = {
            'automotive': {
                'core_domain': [
                    'automotive_engineering', 'vehicle_dynamics', 'powertrain',
                    'electric_vehicles', 'autonomous_driving', 'adas',
                    'can_bus', 'autosar', 'iso_26262', 'automotive_safety'
                ]
            },
            'healthcare': {
                'core_domain': [
                    'clinical_research', 'medical_devices', 'pharmaceutical',
                    'biotechnology', 'healthcare_compliance', 'hipaa', 'fda_regulations',
                    'clinical_trials', 'medical_imaging', 'telemedicine'
                ]
            },
            'finance': {
                'core_domain': [
                    'financial_modeling', 'risk_management', 'trading', 'investment',
                    'banking', 'fintech', 'blockchain', 'cryptocurrency',
                    'regulatory_compliance', 'basel_iii', 'mifid_ii'
                ],
                'regulatory': ['kyc', 'aml', 'gdpr', 'pci_dss', 'sox_compliance']
            },
            'energy': {
                'core_domain': [
                    'renewable_energy', 'solar_energy', 'wind_energy', 'energy_storage',
                    'smart_grid', 'power_systems', 'electrical_engineering',
                    'energy_management', 'sustainability'
                ]
            },
            'aerospace': {
                'core_domain': [
                    'aerospace_engineering', 'avionics', 'flight_systems',
                    'satellite_technology', 'space_systems', 'do_178c',
                    'aviation_safety', 'flight_testing'
                ]
            }
        }
        
        # æŠ€èƒ½æ’é™¤è§„åˆ™ - æŸäº›æŠ€èƒ½ä¸åº”è¯¥åˆ†é…ç»™ç‰¹å®šè§’è‰²
        self.exclusion_rules = {
            'ui_ux_designer': {
                'exclude_skills': ['python', 'java', 'sql', 'backend_development'],
                'exclude_categories': ['programming_language', 'database']
            },
            'digital_marketing_specialist': {
                'exclude_skills': ['python', 'java', 'docker', 'kubernetes'],
                'exclude_categories': ['programming_language', 'cloud_service']
            }
        }
        
        logger.info(f"âœ… åŠ è½½äº† {len(self.explicit_role_mappings)} ä¸ªè§’è‰²å’Œ {len(self.explicit_industry_mappings)} ä¸ªè¡Œä¸šçš„æ˜¾å¼æ˜ å°„")
    
    def _load_semantic_descriptions(self):
        """åŠ è½½è¯­ä¹‰æè¿° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘é‡å """
        # æ›´ç²¾ç¡®çš„è§’è‰²è¯­ä¹‰æè¿°ï¼Œå‡å°‘é‡å 
        self.role_semantic_descriptions = {
            'software_engineer': 'general software development programming coding applications systems architecture',
            'data_scientist': 'data analysis statistics machine learning predictive modeling analytics research insights',
            'product_manager': 'product strategy roadmap requirements business analysis market research user needs',
            'digital_marketing_specialist': 'digital marketing campaigns advertising social media brand promotion customer acquisition',
            'mechanical_engineer': 'mechanical design manufacturing CAD materials physics mechanics systems',
            'electrical_engineer': 'electrical circuits electronics power systems embedded hardware automation control',
            'automotive_engineer': 'vehicle automotive transportation mobility electric autonomous powertrain safety',
            'business_analyst': 'business process analysis requirements gathering stakeholder consultation improvement optimization',
            'project_manager': 'project coordination planning scheduling resource management delivery execution',
            'cybersecurity_specialist': 'security protection threat vulnerability risk assessment compliance audit',
            'devops_engineer': 'deployment automation infrastructure orchestration continuous integration delivery operations',
            'cloud_engineer': 'cloud infrastructure scalability distributed systems serverless containerization',
            'ai_ml_engineer': 'artificial intelligence machine learning deep learning neural networks algorithms',
            'frontend_web_developer': 'user interface web frontend javascript html css responsive interactive',
            'backend_developer': 'server backend api database microservices architecture scalability performance',
            'mobile_app_developer': 'mobile applications ios android native cross-platform mobile user experience',
            'ui_ux_designer': 'user interface user experience design visual interaction usability wireframes prototypes',
            'quality_assurance_engineer': 'testing quality assurance automation bug verification validation',
            'systems_administrator': 'systems administration server infrastructure maintenance monitoring troubleshooting',
            'database_administrator': 'database administration performance optimization backup recovery maintenance'
        }
        
        # æ›´ç²¾ç¡®çš„è¡Œä¸šè¯­ä¹‰æè¿°
        self.industry_semantic_descriptions = {
            'consulting': 'consulting advisory strategy transformation change management business solutions',
            'finance': 'financial banking investment trading wealth management risk assessment regulatory',
            'tech': 'technology software digital innovation programming computer systems information',
            'law': 'legal litigation corporate law intellectual property compliance regulatory affairs',
            'automotive': 'automotive vehicles transportation manufacturing electric autonomous mobility',
            'healthcare': 'healthcare medical clinical pharmaceutical biotechnology patient care devices',
            'media': 'media entertainment content creation broadcasting journalism creative arts',
            'real_estate': 'real estate property development construction architecture management residential commercial',
            'energy': 'energy renewable power sustainability oil gas solar wind nuclear infrastructure',
            'manufacturing': 'manufacturing production industrial operations quality control supply chain',
            'retail': 'retail ecommerce sales customer service merchandising consumer goods',
            'education': 'education learning training academic teaching development research',
            'aerospace': 'aerospace aviation flight space aircraft defense satellite systems',
            'chemicals': 'chemical manufacturing materials petrochemical pharmaceutical industrial chemistry',
            'telecommunications': 'telecommunications networking communication wireless connectivity infrastructure',
            'food': 'food beverage restaurant culinary agriculture nutrition processing hospitality',
            'logistics': 'logistics transportation supply chain shipping warehouse distribution',
            'gaming': 'gaming entertainment interactive esports game development virtual reality',
            'fashion': 'fashion design textile apparel luxury retail brand creative',
            'sports': 'sports fitness athletic recreation wellness health management',
            'agriculture': 'agriculture farming crops livestock sustainable food production',
            'mining': 'mining extraction minerals resources geological engineering',
            'hospitality': 'hospitality hotel travel tourism leisure customer service',
            'security': 'security protection safety risk management surveillance',
            'entertainment': 'entertainment media content film music television arts',
            'government': 'government public policy administration civic services regulatory',
            'nonprofit': 'nonprofit charity social philanthropy community development',
            'biotechnology': 'biotechnology life sciences genetics molecular biology research',
            'insurance': 'insurance risk assessment actuarial underwriting claims protection',
            'architecture': 'architecture design building construction planning urban structural',
            'environmental': 'environmental sustainability green technology climate ecology conservation'
        }
    
    def classify_skills(self, all_skills: List[Dict]) -> Tuple[Dict, Dict]:
        """åˆ†çº§åˆ†ç±»æ‰€æœ‰æŠ€èƒ½"""
        logger.info("ğŸ§  å¼€å§‹åˆ†çº§æ™ºèƒ½æŠ€èƒ½åˆ†ç±»...")
        
        # åˆå§‹åŒ–åˆ†ç±»ç»“æ„
        role_classifications = {role: {
            'essential': [], 'common': [], 'collaboration': [], 'specializations': []
        } for role in self.role_semantic_descriptions.keys()}
        
        industry_classifications = {industry: {
            'core_domain': [], 'regulatory': [], 'business_focus': [], 'unique_requirements': []
        } for industry in self.industry_semantic_descriptions.keys()}
        
        # é¢„è®¡ç®—embeddings
        self._precompute_embeddings()
        
        # ç»Ÿè®¡å˜é‡
        stats = {
            'explicit_role_assigned': 0,
            'explicit_industry_assigned': 0,
            'semantic_role_assigned': 0,
            'semantic_industry_assigned': 0,
            'unassigned': 0,
            'skipped_by_exclusion': 0
        }
        
        # æ”¶é›†å·²åˆ†é…çš„æŠ€èƒ½
        assigned_skills = set()
        
        # === ç¬¬ä¸€è½®ï¼šæ˜¾å¼è§„åˆ™åˆ†é… ===
        logger.info("ğŸ“‹ ç¬¬ä¸€è½®ï¼šæ˜¾å¼è§„åˆ™åˆ†é…...")
        for skill_info in all_skills:
            skill = skill_info['skill']
            
            # è§’è‰²æ˜¾å¼åˆ†é…
            role_assigned = self._assign_explicit_role(skill, role_classifications)
            if role_assigned:
                assigned_skills.add(skill)
                stats['explicit_role_assigned'] += 1
                continue
            
            # è¡Œä¸šæ˜¾å¼åˆ†é…
            industry_assigned = self._assign_explicit_industry(skill, industry_classifications)
            if industry_assigned:
                assigned_skills.add(skill)
                stats['explicit_industry_assigned'] += 1
        
        # === ç¬¬äºŒè½®ï¼šé«˜ç½®ä¿¡åº¦è¯­ä¹‰åŒ¹é… ===
        logger.info("ğŸ¯ ç¬¬äºŒè½®ï¼šé«˜ç½®ä¿¡åº¦è¯­ä¹‰åŒ¹é…...")
        for skill_info in all_skills:
            skill = skill_info['skill']
            
            if skill in assigned_skills:
                continue
                
            # æ£€æŸ¥æ’é™¤è§„åˆ™
            if self._is_excluded_by_rules(skill_info):
                stats['skipped_by_exclusion'] += 1
                continue
            
            # é«˜ç½®ä¿¡åº¦è§’è‰²åŒ¹é…
            best_role, role_similarity = self._calculate_best_role_match(skill_info)
            if best_role and role_similarity >= self.high_confidence_threshold:
                category = self._determine_role_category(skill_info, role_similarity)
                role_classifications[best_role][category].append(skill)
                assigned_skills.add(skill)
                stats['semantic_role_assigned'] += 1
                continue
            
            # é«˜ç½®ä¿¡åº¦è¡Œä¸šåŒ¹é…
            best_industry, industry_similarity = self._calculate_best_industry_match(skill_info)
            if best_industry and industry_similarity >= self.high_confidence_threshold:
                category = self._determine_industry_category(skill_info, industry_similarity)
                industry_classifications[best_industry][category].append(skill)
                assigned_skills.add(skill)
                stats['semantic_industry_assigned'] += 1
        
        # === ç¬¬ä¸‰è½®ï¼šä¸­ç­‰ç½®ä¿¡åº¦è¯­ä¹‰åŒ¹é… ===
        logger.info("ğŸ” ç¬¬ä¸‰è½®ï¼šä¸­ç­‰ç½®ä¿¡åº¦è¯­ä¹‰åŒ¹é…...")
        for skill_info in all_skills:
            skill = skill_info['skill']
            
            if skill in assigned_skills:
                continue
                
            if self._is_excluded_by_rules(skill_info):
                continue
            
            # ä¸­ç­‰ç½®ä¿¡åº¦è§’è‰²åŒ¹é…
            best_role, role_similarity = self._calculate_best_role_match(skill_info)
            if best_role and role_similarity >= self.medium_confidence_threshold:
                category = self._determine_role_category(skill_info, role_similarity)
                role_classifications[best_role][category].append(skill)
                assigned_skills.add(skill)
                stats['semantic_role_assigned'] += 1
                continue
            
            # ä¸­ç­‰ç½®ä¿¡åº¦è¡Œä¸šåŒ¹é…
            best_industry, industry_similarity = self._calculate_best_industry_match(skill_info)
            if best_industry and industry_similarity >= self.medium_confidence_threshold:
                category = self._determine_industry_category(skill_info, industry_similarity)
                industry_classifications[best_industry][category].append(skill)
                assigned_skills.add(skill)
                stats['semantic_industry_assigned'] += 1
        
        # === ç¬¬å››è½®ï¼šå…œåº•å¤„ç† ===
        logger.info("ğŸ¥… ç¬¬å››è½®ï¼šå…œåº•å¤„ç†...")
        for skill_info in all_skills:
            skill = skill_info['skill']
            
            if skill in assigned_skills:
                continue
            
            # å°è¯•ä½ç½®ä¿¡åº¦åŒ¹é…
            best_role, role_similarity = self._calculate_best_role_match(skill_info)
            best_industry, industry_similarity = self._calculate_best_industry_match(skill_info)
            
            # é€‰æ‹©æ›´å¥½çš„åŒ¹é…
            if role_similarity >= self.low_confidence_threshold and role_similarity > industry_similarity:
                category = self._determine_role_category(skill_info, role_similarity)
                role_classifications[best_role][category].append(skill)
                stats['semantic_role_assigned'] += 1
            elif industry_similarity >= self.low_confidence_threshold:
                category = self._determine_industry_category(skill_info, industry_similarity)
                industry_classifications[best_industry][category].append(skill)
                stats['semantic_industry_assigned'] += 1
            else:
                # æœ€ç»ˆå…œåº•åˆ°techè¡Œä¸š
                industry_classifications['tech']['core_domain'].append(skill)
                stats['unassigned'] += 1
        
        # æ¸…ç†ç©ºåˆ†ç±»
        role_classifications = self._clean_classifications(role_classifications)
        industry_classifications = self._clean_classifications(industry_classifications)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        total_processed = sum(stats.values())
        logger.info(f"ğŸ“Š åˆ†çº§åˆ†ç±»å®Œæˆ:")
        logger.info(f"  ğŸ“‹ æ˜¾å¼è§’è‰²åˆ†é…: {stats['explicit_role_assigned']}")
        logger.info(f"  ğŸ¢ æ˜¾å¼è¡Œä¸šåˆ†é…: {stats['explicit_industry_assigned']}")
        logger.info(f"  ğŸ¯ è¯­ä¹‰è§’è‰²åˆ†é…: {stats['semantic_role_assigned']}")
        logger.info(f"  ğŸ­ è¯­ä¹‰è¡Œä¸šåˆ†é…: {stats['semantic_industry_assigned']}")
        logger.info(f"  ğŸ¥… å…œåº•åˆ†é…: {stats['unassigned']}")
        logger.info(f"  âŒ æ’é™¤è·³è¿‡: {stats['skipped_by_exclusion']}")
        logger.info(f"  ğŸ“ˆ æ€»å¤„ç†: {len(all_skills)} ä¸ªæŠ€èƒ½")
        
        return role_classifications, industry_classifications
    
    def _precompute_embeddings(self):
        """é¢„è®¡ç®—embeddings"""
        logger.info("ğŸ§¬ é¢„è®¡ç®—embeddingå‘é‡...")
        
        self.role_embeddings = {}
        for role, description in self.role_semantic_descriptions.items():
            self.role_embeddings[role] = self.model.encode([description])[0]
        
        self.industry_embeddings = {}
        for industry, description in self.industry_semantic_descriptions.items():
            self.industry_embeddings[industry] = self.model.encode([description])[0]
        
        logger.info(f"âœ… é¢„è®¡ç®—å®Œæˆ: {len(self.role_embeddings)} è§’è‰², {len(self.industry_embeddings)} è¡Œä¸š")
    
    def _calculate_best_role_match(self, skill_info: Dict) -> Tuple[Optional[str], float]:
        """è®¡ç®—æœ€ä½³è§’è‰²åŒ¹é…"""
        enhanced_skill = self.enhance_skill_description(skill_info)
        
        try:
            skill_embedding = self.model.encode([enhanced_skill])[0]
        except Exception:
            return None, 0.0
        
        best_role, best_similarity = None, 0.0
        
        for role, role_embedding in self.role_embeddings.items():
            similarity = cosine_similarity([skill_embedding], [role_embedding])[0][0]
            bonus = self._calculate_role_type_bonus(skill_info, role)
            adjusted_similarity = similarity * (1 + bonus)
            
            if adjusted_similarity > best_similarity:
                best_similarity = adjusted_similarity
                best_role = role
        
        return best_role, best_similarity
    
    def _calculate_best_industry_match(self, skill_info: Dict) -> Tuple[Optional[str], float]:
        """è®¡ç®—æœ€ä½³è¡Œä¸šåŒ¹é…"""
        enhanced_skill = self.enhance_skill_description(skill_info)
        
        try:
            skill_embedding = self.model.encode([enhanced_skill])[0]
        except Exception:
            return None, 0.0
        
        best_industry, best_similarity = None, 0.0
        
        for industry, industry_embedding in self.industry_embeddings.items():
            similarity = cosine_similarity([skill_embedding], [industry_embedding])[0][0]
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_industry = industry
        
        return best_industry, best_similarity
    
    def _calculate_role_type_bonus(self, skill_info: Dict, role: str) -> float:
        """è®¡ç®—è§’è‰²ç±»å‹åŠ åˆ† - é™ä½åŠ åˆ†å¹…åº¦"""
        skill_type = skill_info['skill_type']
        skill = skill_info['skill'].lower()
        
        # å‡å°‘åŠ åˆ†å¹…åº¦ï¼Œé¿å…è¿‡åº¦åå‘æŸäº›è§’è‰²
        role_type_affinity = {
            'software_engineer': ['programming_language', 'framework'],
            'data_scientist': ['programming_language', 'tool'],
            'devops_engineer': ['cloud_service', 'tool'],
            'cloud_engineer': ['cloud_service'],
            'frontend_web_developer': ['programming_language', 'framework'],
            'backend_developer': ['programming_language', 'database'],
            'ui_ux_designer': ['tool'],
            'project_manager': ['methodology'],
            'cybersecurity_specialist': ['certification', 'tool']
        }
        
        # é™ä½åŠ åˆ†æ¯”ä¾‹ä»0.2é™åˆ°0.1 (10%)
        if role in role_type_affinity and skill_type in role_type_affinity[role]:
            return 0.1
        
        # ç‰¹æ®ŠæŠ€èƒ½ç›´æ¥åŒ¹é… - ä¹Ÿé™ä½åŠ åˆ†
        special_matches = {
            'figma': ['ui_ux_designer'],
            'sketch': ['ui_ux_designer'],
            'adobe_xd': ['ui_ux_designer'],
            'google_ads': ['digital_marketing_specialist'],
            'facebook_ads': ['digital_marketing_specialist'],
            'jira': ['project_manager'],
            'scrum': ['project_manager'],
            'agile': ['project_manager'],
            'tableau': ['data_scientist'],
            'kubernetes': ['devops_engineer', 'cloud_engineer'],
            'docker': ['devops_engineer', 'cloud_engineer']
        }
        
        # é™ä½ç‰¹æ®ŠåŒ¹é…åŠ åˆ†ä»0.3é™åˆ°0.15 (15%)
        if skill in special_matches and role in special_matches[skill]:
            return 0.15
        
        return 0.0
    
    def _determine_role_category(self, skill_info: Dict, similarity: float) -> str:
        """ç¡®å®šè§’è‰²ç±»åˆ« - æé«˜é˜ˆå€¼"""
        skill_type = skill_info['skill_type']
        
        # æé«˜å„ä¸ªé˜ˆå€¼ï¼Œä½¿åˆ†ç±»æ›´åŠ ä¸¥æ ¼
        if similarity >= 0.85:  # ä»0.8æé«˜åˆ°0.85
            return 'essential'
        elif similarity >= 0.75:  # ä»0.7æé«˜åˆ°0.75
            return 'essential' if skill_type in ['programming_language', 'framework', 'database'] else 'common'
        elif similarity >= 0.65:  # ä»0.6æé«˜åˆ°0.65
            return 'specializations' if skill_type in ['certification', 'methodology'] else 'common'
        else:
            return 'collaboration'
    
    def _determine_industry_category(self, skill_info: Dict, similarity: float) -> str:
        """ç¡®å®šè¡Œä¸šç±»åˆ«"""
        skill_type = skill_info['skill_type']
        skill = skill_info['skill'].lower()
        
        if skill_type == 'certification' or skill_type == 'language':
            return 'unique_requirements'
        elif 'compliance' in skill or 'regulatory' in skill or 'gdpr' in skill or 'iso' in skill:
            return 'regulatory'
        elif skill_type == 'methodology' or 'management' in skill or 'strategy' in skill:
            return 'business_focus'
        else:
            return 'core_domain'
    
    def _clean_classifications(self, classifications: Dict) -> Dict:
        """æ¸…ç†åˆ†ç±»ç»“æœ"""
        cleaned = {}
        for key, categories in classifications.items():
            cleaned_categories = {}
            for category_name, skills in categories.items():
                if isinstance(skills, list) and skills:
                    unique_skills = sorted(list(set(skills)))
                    cleaned_categories[category_name] = unique_skills
            if cleaned_categories:
                cleaned[key] = cleaned_categories
        return cleaned

class BaselineGenerator:
    """è¯­ä¹‰åŸºçº¿æ„å»ºå™¨"""
    
    def __init__(self, config: PipelineConfig, classification_model: SentenceTransformer):
        self.config = config
        self.classification_model = classification_model  # ç”¨äºåˆ†ç±»çš„æ¨¡å‹
        
        # åˆå§‹åŒ–ä¸“é—¨ç”¨äºå‘é‡ç”Ÿæˆçš„æ¨¡å‹
        logger.info("ğŸ”§ åˆå§‹åŒ–å‘é‡ç”Ÿæˆæ¨¡å‹...")
        try:
            self.vector_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("âœ… æˆåŠŸåŠ è½½å‘é‡ç”Ÿæˆæ¨¡å‹: all-MiniLM-L6-v2")
        except Exception as e:
            logger.error(f"âŒ æ— æ³•åŠ è½½å‘é‡ç”Ÿæˆæ¨¡å‹: {e}")
            raise
    
    def generate_all_baselines(self, role_classifications: Dict, industry_classifications: Dict) -> Dict:
        """ç”Ÿæˆæ‰€æœ‰åŸºçº¿ï¼ˆåˆ†ç¦»è¯­ä¹‰æ•°æ®å’Œå‘é‡æ•°æ®ï¼‰"""
        logger.info("ğŸ—ï¸ å¼€å§‹ç”Ÿæˆè¯­ä¹‰åŸºçº¿...")
        
        baselines = {
            'global': self._generate_global_baseline(role_classifications, industry_classifications),
            'roles': self._generate_role_baselines(role_classifications),
            'industries': self._generate_industry_baselines(industry_classifications)
        }
        
        logger.info("âœ… æ‰€æœ‰åŸºçº¿ç”Ÿæˆå®Œæˆ")
        return baselines
    
    def _generate_global_baseline(self, role_classifications: Dict, industry_classifications: Dict) -> Dict:
        """ç”Ÿæˆå…¨å±€åŸºçº¿"""
        logger.info("ğŸŒ ç”Ÿæˆå…¨å±€åŸºçº¿...")
        
        all_skills = set()
        for categories in role_classifications.values():
            for skills in categories.values():
                all_skills.update(skills)
        
        for categories in industry_classifications.values():
            for skills in categories.values():
                all_skills.update(skills)
        
        all_skills = list(all_skills)
        
        # ç»Ÿè®¡ä¿¡æ¯
        role_distribution = {role: sum(len(skills) for skills in categories.values()) 
                           for role, categories in role_classifications.items()}
        industry_distribution = {industry: sum(len(skills) for skills in categories.values()) 
                               for industry, categories in industry_classifications.items()}
        
        # åˆ†ç¦»è¯­ä¹‰æ•°æ®å’Œå‘é‡æ•°æ®
        semantic_data = {
            'total_skills': len(all_skills),
            'skills_list': sorted(all_skills),  # æŠ€èƒ½åˆ—è¡¨ï¼ˆäººç±»å¯è¯»ï¼‰
            'statistics': {
                'role_distribution': role_distribution,
                'industry_distribution': industry_distribution,
                'total_roles': len(role_classifications),
                'total_industries': len(industry_classifications)
            },
            'metadata': {
                'generation_date': datetime.now().isoformat(),
                'classification_model': self.config.classification_model,
                'vector_model': 'all-MiniLM-L6-v2',
                'embedding_dimension': 384  # all-MiniLM-L6-v2 çš„ç»´åº¦
            }
        }
        
        # ç”Ÿæˆå‘é‡æ•°æ®
        vector_data = self._generate_vectors_for_skills(all_skills)
        
        return {
            'semantic_data': semantic_data,
            'vector_data': vector_data
        }
    
    def _generate_role_baselines(self, role_classifications: Dict) -> Dict:
        """ç”Ÿæˆè§’è‰²åŸºçº¿"""
        logger.info("ğŸ‘” ç”Ÿæˆè§’è‰²åŸºçº¿...")
        
        role_baselines = {}
        
        for role, categories in role_classifications.items():
            all_role_skills = []
            for skills in categories.values():
                all_role_skills.extend(skills)
            
            if len(all_role_skills) < self.config.min_skills_for_baseline:
                logger.warning(f"è§’è‰² {role} æŠ€èƒ½æ•°é‡ä¸è¶³ï¼Œè·³è¿‡åŸºçº¿ç”Ÿæˆ")
                continue
            
            # è¯­ä¹‰æ•°æ®ï¼ˆäººç±»å¯è¯»ï¼‰
            semantic_data = {
                'role_name': role,
                'categories': categories,
                'total_skills': len(all_role_skills),
                'skills_list': sorted(list(set(all_role_skills))),
                'statistics': {
                    'category_distribution': {cat: len(skills) for cat, skills in categories.items() if skills}
                },
                'metadata': {
                    'generation_date': datetime.now().isoformat(),
                    'classification_model': self.config.classification_model,
                    'vector_model': 'all-MiniLM-L6-v2'
                }
            }
            
            # å‘é‡æ•°æ®
            vector_data = self._generate_vectors_for_role(categories)
            
            role_baselines[role] = {
                'semantic_data': semantic_data,
                'vector_data': vector_data
            }
        
        return role_baselines
    
    def _generate_industry_baselines(self, industry_classifications: Dict) -> Dict:
        """ç”Ÿæˆè¡Œä¸šåŸºçº¿"""
        logger.info("ğŸ­ ç”Ÿæˆè¡Œä¸šåŸºçº¿...")
        
        industry_baselines = {}
        
        for industry, categories in industry_classifications.items():
            all_industry_skills = []
            for skills in categories.values():
                all_industry_skills.extend(skills)
            
            if len(all_industry_skills) < self.config.min_skills_for_baseline:
                logger.warning(f"è¡Œä¸š {industry} æŠ€èƒ½æ•°é‡ä¸è¶³ï¼Œè·³è¿‡åŸºçº¿ç”Ÿæˆ")
                continue
            
            # è¯­ä¹‰æ•°æ®ï¼ˆäººç±»å¯è¯»ï¼‰
            semantic_data = {
                'industry_name': industry,
                'categories': categories,
                'total_skills': len(all_industry_skills),
                'skills_list': sorted(list(set(all_industry_skills))),
                'statistics': {
                    'category_distribution': {cat: len(skills) for cat, skills in categories.items() if skills}
                },
                'metadata': {
                    'generation_date': datetime.now().isoformat(),
                    'classification_model': self.config.classification_model,
                    'vector_model': 'all-MiniLM-L6-v2'
                }
            }
            
            # å‘é‡æ•°æ®
            vector_data = self._generate_vectors_for_industry(categories)
            
            industry_baselines[industry] = {
                'semantic_data': semantic_data,
                'vector_data': vector_data
            }
        
        return industry_baselines
    
    def _generate_vectors_for_skills(self, skills: List[str]) -> Dict:
        """ä¸ºæŠ€èƒ½åˆ—è¡¨ç”Ÿæˆå‘é‡æ•°æ®"""
        if not skills:
            return {}
        
        try:
            # ä½¿ç”¨å‘é‡ç”Ÿæˆæ¨¡å‹è®¡ç®—embeddings
            skill_embeddings = self.vector_model.encode(skills)
            
            # è®¡ç®—å…¨å±€å‘é‡ï¼ˆå¹³å‡å‘é‡ï¼‰
            global_vector = np.mean(skill_embeddings, axis=0) if len(skill_embeddings) > 0 else np.zeros(384)
            
            return {
                'global_vector': global_vector.tolist(),
                'skill_vectors': {skill: embedding.tolist() for skill, embedding in zip(skills, skill_embeddings)},
                'vector_metadata': {
                    'total_vectors': len(skills),
                    'vector_dimension': 384,
                    'model_used': 'all-MiniLM-L6-v2'
                }
            }
        except Exception as e:
            logger.error(f"å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return {}
    
    def _generate_vectors_for_role(self, categories: Dict) -> Dict:
        """ä¸ºè§’è‰²åˆ†ç±»ç”Ÿæˆå‘é‡æ•°æ®"""
        all_skills = []
        for skills in categories.values():
            all_skills.extend(skills)
        
        if not all_skills:
            return {}
        
        try:
            # ç”Ÿæˆæ‰€æœ‰æŠ€èƒ½çš„å‘é‡
            unique_skills = list(set(all_skills))
            skill_embeddings = self.vector_model.encode(unique_skills)
            skill_vectors = {skill: embedding.tolist() for skill, embedding in zip(unique_skills, skill_embeddings)}
            
            # è®¡ç®—å„ç±»åˆ«çš„å¹³å‡å‘é‡
            category_vectors = {}
            for category, skills in categories.items():
                if skills:
                    category_embeddings = [self.vector_model.encode([skill])[0] for skill in skills]
                    if category_embeddings:
                        category_vectors[category] = np.mean(category_embeddings, axis=0).tolist()
            
            # è®¡ç®—æ•´ä½“è§’è‰²å‘é‡
            role_vector = np.mean(skill_embeddings, axis=0) if len(skill_embeddings) > 0 else np.zeros(384)
            
            return {
                'role_vector': role_vector.tolist(),
                'category_vectors': category_vectors,
                'skill_vectors': skill_vectors,
                'vector_metadata': {
                    'total_vectors': len(unique_skills),
                    'vector_dimension': 384,
                    'model_used': 'all-MiniLM-L6-v2'
                }
            }
        except Exception as e:
            logger.error(f"è§’è‰²å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return {}
    
    def _generate_vectors_for_industry(self, categories: Dict) -> Dict:
        """ä¸ºè¡Œä¸šåˆ†ç±»ç”Ÿæˆå‘é‡æ•°æ®"""
        all_skills = []
        for skills in categories.values():
            all_skills.extend(skills)
        
        if not all_skills:
            return {}
        
        try:
            # ç”Ÿæˆæ‰€æœ‰æŠ€èƒ½çš„å‘é‡
            unique_skills = list(set(all_skills))
            skill_embeddings = self.vector_model.encode(unique_skills)
            skill_vectors = {skill: embedding.tolist() for skill, embedding in zip(unique_skills, skill_embeddings)}
            
            # è®¡ç®—å„ç±»åˆ«çš„å¹³å‡å‘é‡
            category_vectors = {}
            for category, skills in categories.items():
                if skills:
                    category_embeddings = [self.vector_model.encode([skill])[0] for skill in skills]
                    if category_embeddings:
                        category_vectors[category] = np.mean(category_embeddings, axis=0).tolist()
            
            # è®¡ç®—æ•´ä½“è¡Œä¸šå‘é‡
            industry_vector = np.mean(skill_embeddings, axis=0) if len(skill_embeddings) > 0 else np.zeros(384)
            
            return {
                'industry_vector': industry_vector.tolist(),
                'category_vectors': category_vectors,
                'skill_vectors': skill_vectors,
                'vector_metadata': {
                    'total_vectors': len(unique_skills),
                    'vector_dimension': 384,
                    'model_used': 'all-MiniLM-L6-v2'
                }
            }
        except Exception as e:
            logger.error(f"è¡Œä¸šå‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return {}

class DatabaseManager:
    """æ•°æ®åº“æ“ä½œç®¡ç†å™¨ - åˆ†ç¦»è¯­ä¹‰æ•°æ®å’Œå‘é‡æ•°æ®"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.supabase = self._init_supabase()
    
    def _init_supabase(self) -> Optional[Client]:
        """åˆå§‹åŒ–Supabaseå®¢æˆ·ç«¯"""
        if not self.config.supabase_url or not self.config.supabase_key:
            logger.warning("âš ï¸ Supabaseé…ç½®ç¼ºå¤±ï¼Œè·³è¿‡æ•°æ®åº“æ“ä½œ")
            return None
        
        try:
            client = create_client(self.config.supabase_url, self.config.supabase_key)
            logger.info("âœ… Supabaseå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return client
        except Exception as e:
            logger.error(f"âŒ Supabaseå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def upload_baselines(self, baselines: Dict) -> Dict:
        """ä¸Šä¼ åŸºçº¿æ•°æ®åˆ°æ•°æ®åº“ï¼ˆåˆ†ç¦»å­˜å‚¨ï¼‰"""
        if not self.supabase:
            logger.warning("âš ï¸ æ•°æ®åº“æœªè¿æ¥ï¼Œè·³è¿‡ä¸Šä¼ ")
            return {'status': 'skipped', 'reason': 'no_database_connection'}
        
        logger.info("ğŸ“¤ å¼€å§‹ä¸Šä¼ åŸºçº¿æ•°æ®åˆ°åˆ†ç¦»çš„è¡¨...")
        results = {}
        
        # ä¸Šä¼ å…¨å±€åŸºçº¿
        results['global'] = self._upload_baseline_pair(
            'global', 'global_baseline', baselines['global']
        )
        
        # ä¸Šä¼ è§’è‰²åŸºçº¿
        results['roles'] = {}
        for role, baseline_data in baselines['roles'].items():
            results['roles'][role] = self._upload_baseline_pair(
                'role', f'{role}_baseline', baseline_data
            )
        
        # ä¸Šä¼ è¡Œä¸šåŸºçº¿
        results['industries'] = {}
        for industry, baseline_data in baselines['industries'].items():
            results['industries'][industry] = self._upload_baseline_pair(
                'industry', f'{industry}_baseline', baseline_data
            )
        
        logger.info("âœ… åŸºçº¿æ•°æ®ä¸Šä¼ å®Œæˆ")
        return results
    
    def _upload_baseline_pair(self, baseline_type: str, name: str, baseline_data: Dict) -> Dict:
        """ä¸Šä¼ åŸºçº¿æ•°æ®å¯¹ï¼ˆè¯­ä¹‰æ•°æ® + å‘é‡æ•°æ®ï¼‰"""
        try:
            semantic_data = baseline_data.get('semantic_data', {})
            vector_data = baseline_data.get('vector_data', {})
            
            if not semantic_data:
                logger.warning(f"åŸºçº¿ {name} ç¼ºå°‘è¯­ä¹‰æ•°æ®ï¼Œè·³è¿‡ä¸Šä¼ ")
                return {'status': 'error', 'error': 'missing_semantic_data'}
            
            # 1. é¦–å…ˆä¸Šä¼ è¯­ä¹‰æ•°æ®åˆ° semantic_baselines è¡¨
            semantic_result = self._upload_semantic_baseline(baseline_type, name, semantic_data)
            
            if semantic_result['status'] != 'uploaded':
                logger.error(f"è¯­ä¹‰æ•°æ®ä¸Šä¼ å¤±è´¥ï¼Œè·³è¿‡å‘é‡æ•°æ®ä¸Šä¼ : {name}")
                return semantic_result
            
            baseline_id = semantic_result['id']
            
            # 2. ç„¶åä¸Šä¼ å‘é‡æ•°æ®åˆ° baseline_vectors è¡¨
            if vector_data:
                vector_result = self._upload_vector_data(baseline_id, vector_data)
                
                return {
                    'status': 'uploaded',
                    'baseline_id': baseline_id,
                    'semantic_upload': semantic_result,
                    'vector_upload': vector_result
                }
            else:
                logger.warning(f"åŸºçº¿ {name} ç¼ºå°‘å‘é‡æ•°æ®")
                return {
                    'status': 'partial',
                    'baseline_id': baseline_id,
                    'semantic_upload': semantic_result,
                    'vector_upload': {'status': 'skipped', 'reason': 'no_vector_data'}
                }
                
        except Exception as e:
            logger.error(f"ä¸Šä¼ åŸºçº¿å¯¹ {name} å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _upload_semantic_baseline(self, baseline_type: str, name: str, semantic_data: Dict) -> Dict:
        """ä¸Šä¼ è¯­ä¹‰æ•°æ®åˆ° semantic_baselines è¡¨"""
        source_hash = self._calculate_hash(semantic_data)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„è¯­ä¹‰æ•°æ®
        try:
            result = self.supabase.table("semantic_baselines") \
                .select("*") \
                .eq("source_hash", source_hash) \
                .execute()
            
            if result.data:
                logger.info(f"è¯­ä¹‰åŸºçº¿ {name} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ ")
                return {'status': 'exists', 'id': result.data[0]['id']}
        except Exception as e:
            logger.error(f"æ£€æŸ¥è¯­ä¹‰åŸºçº¿ {name} æ—¶å‡ºé”™: {e}")
            return {'status': 'error', 'error': str(e)}
        
        # æ’å…¥æ–°çš„è¯­ä¹‰æ•°æ®
        insert_data = {
            "baseline_type": baseline_type,
            "name": name,
            "version": 1,
            "is_active": True,
            "baseline_data": semantic_data,  # åªåŒ…å«äººç±»å¯è¯»çš„æ•°æ®ï¼Œä¸åŒ…å«å‘é‡
            "source_hash": source_hash
        }
        
        try:
            result = self.supabase.table("semantic_baselines") \
                .insert(insert_data) \
                .execute()
            
            logger.info(f"âœ… æˆåŠŸä¸Šä¼ è¯­ä¹‰åŸºçº¿: {name}")
            return {'status': 'uploaded', 'id': result.data[0]['id']}
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ è¯­ä¹‰åŸºçº¿ {name} å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _upload_vector_data(self, baseline_id: int, vector_data: Dict) -> Dict:
        """ä¸Šä¼ å‘é‡æ•°æ®åˆ° baseline_vectors è¡¨"""
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥baseline_idçš„å‘é‡æ•°æ®
        try:
            result = self.supabase.table("baseline_vectors") \
                .select("*") \
                .eq("baseline_id", baseline_id) \
                .eq("embedding_model", "all-MiniLM-L6-v2") \
                .execute()
            
            if result.data:
                logger.info(f"å‘é‡æ•°æ® baseline_id={baseline_id} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ ")
                return {'status': 'exists', 'id': result.data[0]['id']}
        except Exception as e:
            logger.error(f"æ£€æŸ¥å‘é‡æ•°æ® baseline_id={baseline_id} æ—¶å‡ºé”™: {e}")
            return {'status': 'error', 'error': str(e)}
        
        # æ’å…¥æ–°çš„å‘é‡æ•°æ®
        insert_data = {
            "baseline_id": baseline_id,
            "embedding_model": "all-MiniLM-L6-v2",
            "vectors_data": vector_data  # åŒ…å«æ‰€æœ‰å‘é‡ç›¸å…³æ•°æ®
        }
        
        try:
            result = self.supabase.table("baseline_vectors") \
                .insert(insert_data) \
                .execute()
            
            logger.info(f"âœ… æˆåŠŸä¸Šä¼ å‘é‡æ•°æ®: baseline_id={baseline_id}")
            return {'status': 'uploaded', 'id': result.data[0]['id']}
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ å‘é‡æ•°æ® baseline_id={baseline_id} å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _calculate_hash(self, data: dict) -> str:
        """è®¡ç®—æ•°æ®å“ˆå¸Œå€¼"""
        return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()

class PipelineOrchestrator:
    """æ•´ä½“æµç¨‹ç¼–æ’å™¨"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.processor = SkillProcessor()
        self.classifier = SemanticClassifier(config)
        # ä¿®æ”¹åˆå§‹åŒ–åŸºçº¿ç”Ÿæˆå™¨çš„è°ƒç”¨
        self.baseline_generator = BaselineGenerator(config, self.classifier.model)
        self.database_manager = DatabaseManager(config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.config.output_dir).mkdir(exist_ok=True)
    
    def run_complete_pipeline(self, vocab_path: str) -> Dict:
        """è¿è¡Œå®Œæ•´ç®¡çº¿"""
        logger.info("ğŸš€ å¯åŠ¨ç«¯åˆ°ç«¯æŠ€èƒ½å¤„ç†ç®¡çº¿")
        logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {vocab_path}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.config.output_dir}")
        print("-" * 60)
        
        pipeline_start = datetime.now()
        results = {}
        
        try:
            # 1. åŠ è½½å’Œå¤„ç†è¯æ±‡è¡¨
            logger.info("ğŸ“Š Step 1: åŠ è½½æŠ€èƒ½è¯æ±‡è¡¨...")
            skills_vocab = self._load_vocabulary(vocab_path)
            all_skills = self.processor.extract_all_skills(skills_vocab)
            logger.info(f"ğŸ“‹ æå–åˆ° {len(all_skills)} ä¸ªæŠ€èƒ½")
            results['skills_extracted'] = len(all_skills)
            
            # 2. è¯­ä¹‰åˆ†ç±»
            logger.info("ğŸ§  Step 2: æ‰§è¡Œè¯­ä¹‰åˆ†ç±»...")
            role_classifications, industry_classifications = self.classifier.classify_skills(all_skills)
            results['role_classifications'] = role_classifications
            results['industry_classifications'] = industry_classifications
            
            # ä¿å­˜åˆ†ç±»ç»“æœ
            self._save_classifications(role_classifications, industry_classifications)
            
            # 3. ç”Ÿæˆè¯­ä¹‰åŸºçº¿
            logger.info("ğŸ—ï¸ Step 3: ç”Ÿæˆè¯­ä¹‰åŸºçº¿...")
            baselines = self.baseline_generator.generate_all_baselines(
                role_classifications, industry_classifications
            )
            results['baselines'] = baselines
            
            # ä¿å­˜åŸºçº¿æ–‡ä»¶
            self._save_baselines(baselines)
            
            # 4. ä¸Šä¼ åˆ°æ•°æ®åº“
            logger.info("ğŸ“¤ Step 4: ä¸Šä¼ åˆ°æ•°æ®åº“...")
            upload_results = self.database_manager.upload_baselines(baselines)
            results['upload_results'] = upload_results
            
            # 5. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            pipeline_end = datetime.now()
            duration = (pipeline_end - pipeline_start).total_seconds()
            
            results['summary'] = self._generate_summary_report(
                all_skills, role_classifications, industry_classifications, 
                baselines, upload_results, duration
            )
            
            # ä¿å­˜æ€»ç»“æŠ¥å‘Š
            self._save_summary_report(results['summary'])
            
            logger.info("ğŸ‰ ç®¡çº¿æ‰§è¡Œå®Œæˆ!")
            self._print_summary(results['summary'])
            
        except Exception as e:
            logger.error(f"âŒ ç®¡çº¿æ‰§è¡Œå¤±è´¥: {e}")
            results['error'] = str(e)
            raise
        
        return results
    
    def _load_vocabulary(self, vocab_path: str) -> Dict:
        """åŠ è½½æŠ€èƒ½è¯æ±‡è¡¨"""
        vocab_path = Path(vocab_path)
        if not vocab_path.exists():
            raise FileNotFoundError(f"è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}")
        
        with open(vocab_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # å°è¯•ç›´æ¥è§£æJSON
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä»markdownä¸­æå–JSON
            import re
            json_match = re.search(r'\{[\s\S]*?\}(?=\s*###|\s*$)', content)
            if json_match:
                return json.loads(json_match.group())
            else:
                raise ValueError("æ— æ³•è§£ææŠ€èƒ½è¯æ±‡è¡¨æ–‡ä»¶")
    
    def _save_classifications(self, role_classifications: Dict, industry_classifications: Dict):
        """ä¿å­˜åˆ†ç±»ç»“æœ"""
        # ä¿å­˜è§’è‰²åˆ†ç±»
        role_path = Path(self.config.output_dir) / "role_skill_classifications.json"
        with open(role_path, 'w', encoding='utf-8') as f:
            json.dump(role_classifications, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è§’è‰²åˆ†ç±»å·²ä¿å­˜: {role_path}")
        
        # ä¿å­˜è¡Œä¸šåˆ†ç±»
        industry_path = Path(self.config.output_dir) / "industry_skill_classifications.json"
        with open(industry_path, 'w', encoding='utf-8') as f:
            json.dump(industry_classifications, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è¡Œä¸šåˆ†ç±»å·²ä¿å­˜: {industry_path}")
        
        # ä¿å­˜åˆ†ç±»è¯¦æƒ…ï¼ˆå¦‚æœå¯ç”¨è°ƒè¯•ï¼‰
        if self.config.save_classification_details:
            self._save_classification_details(role_classifications, industry_classifications)
    
    def _save_classification_details(self, role_classifications: Dict, industry_classifications: Dict):
        """ä¿å­˜åˆ†ç±»è¯¦æƒ…ç»Ÿè®¡"""
        details = {
            'classification_summary': {
                'total_roles_with_skills': len([r for r in role_classifications.values() if any(skills for skills in r.values())]),
                'total_industries_with_skills': len([i for i in industry_classifications.values() if any(skills for skills in i.values())]),
                'role_skill_distribution': {
                    role: {cat: len(skills) for cat, skills in categories.items() if skills}
                    for role, categories in role_classifications.items()
                    if any(skills for skills in categories.values())
                },
                'industry_skill_distribution': {
                    industry: {cat: len(skills) for cat, skills in categories.items() if skills}
                    for industry, categories in industry_classifications.items()
                    if any(skills for skills in categories.values())
                }
            },
            'quality_insights': {
                'largest_role_categories': self._get_largest_categories(role_classifications),
                'largest_industry_categories': self._get_largest_categories(industry_classifications),
                'potential_issues': self._identify_potential_issues(role_classifications, industry_classifications)
            },
            'configuration_used': {
                'high_confidence_threshold': self.config.high_confidence_threshold,
                'medium_confidence_threshold': self.config.medium_confidence_threshold,
                'low_confidence_threshold': self.config.low_confidence_threshold,
                'model_name': self.config.classification_model
            }
        }
        
        details_path = Path(self.config.output_dir) / "classification_details.json"
        with open(details_path, 'w', encoding='utf-8') as f:
            json.dump(details, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“‹ åˆ†ç±»è¯¦æƒ…å·²ä¿å­˜: {details_path}")
    
    def _get_largest_categories(self, classifications: Dict) -> List[Dict]:
        """è·å–æœ€å¤§çš„åˆ†ç±»ç±»åˆ«"""
        largest = []
        for name, categories in classifications.items():
            for category, skills in categories.items():
                if skills:
                    largest.append({
                        'name': name,
                        'category': category,
                        'skill_count': len(skills),
                        'sample_skills': skills[:5]  # åªä¿å­˜å‰5ä¸ªæŠ€èƒ½ä½œä¸ºç¤ºä¾‹
                    })
        
        # æŒ‰æŠ€èƒ½æ•°é‡æ’åº
        largest.sort(key=lambda x: x['skill_count'], reverse=True)
        return largest[:10]  # è¿”å›å‰10ä¸ªæœ€å¤§çš„ç±»åˆ«
    
    def _identify_potential_issues(self, role_classifications: Dict, industry_classifications: Dict) -> List[str]:
        """è¯†åˆ«æ½œåœ¨çš„åˆ†ç±»é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è§’è‰²å ç”¨è¿‡å¤šæŠ€èƒ½
        total_role_skills = sum(sum(len(skills) for skills in categories.values()) 
                              for categories in role_classifications.values())
        
        for role, categories in role_classifications.items():
            role_skills = sum(len(skills) for skills in categories.values())
            if role_skills > total_role_skills * 0.3:  # å¦‚æœå•ä¸ªè§’è‰²å ç”¨è¶…è¿‡30%çš„æŠ€èƒ½
                issues.append(f"è§’è‰² '{role}' å¯èƒ½å ç”¨äº†è¿‡å¤šæŠ€èƒ½ ({role_skills} ä¸ª, {role_skills/total_role_skills*100:.1f}%)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¡Œä¸šä¸ºç©º
        empty_industries = [industry for industry, categories in industry_classifications.items() 
                          if not any(skills for skills in categories.values())]
        if len(empty_industries) > len(industry_classifications) * 0.5:
            issues.append(f"è¶…è¿‡ä¸€åŠçš„è¡Œä¸šåˆ†ç±»ä¸ºç©º ({len(empty_industries)}/{len(industry_classifications)})")
        
        # æ£€æŸ¥ç‰¹å®šæŠ€èƒ½æ˜¯å¦è¢«æ­£ç¡®åˆ†ç±»
        design_tools = ['figma', 'sketch', 'adobe_xd']
        ui_ux_skills = []
        if 'ui_ux_designer' in role_classifications:
            ui_ux_skills = [skill for category in role_classifications['ui_ux_designer'].values() 
                           for skill in category]
        
        missing_design_tools = [tool for tool in design_tools if tool not in ui_ux_skills]
        if missing_design_tools:
            issues.append(f"è®¾è®¡å·¥å…·å¯èƒ½æœªæ­£ç¡®åˆ†ç±»åˆ°UI/UXè®¾è®¡å¸ˆ: {missing_design_tools}")
        
        return issues
    
    def _save_baselines(self, baselines: Dict):
        """ä¿å­˜åŸºçº¿æ–‡ä»¶ï¼ˆåˆ†ç¦»è¯­ä¹‰å’Œå‘é‡æ•°æ®ï¼‰"""
        # ä¿å­˜å…¨å±€åŸºçº¿ - è¯­ä¹‰æ•°æ®
        global_semantic_path = Path(self.config.output_dir) / "global_baseline_semantic.json"
        with open(global_semantic_path, 'w', encoding='utf-8') as f:
            json.dump(baselines['global']['semantic_data'], f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ å…¨å±€è¯­ä¹‰åŸºçº¿å·²ä¿å­˜: {global_semantic_path}")
        
        # ä¿å­˜å…¨å±€åŸºçº¿ - å‘é‡æ•°æ®
        global_vector_path = Path(self.config.output_dir) / "global_baseline_vectors.json"
        with open(global_vector_path, 'w', encoding='utf-8') as f:
            json.dump(baselines['global']['vector_data'], f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ å…¨å±€å‘é‡åŸºçº¿å·²ä¿å­˜: {global_vector_path}")
        
        # ä¿å­˜è§’è‰²åŸºçº¿ - è¯­ä¹‰æ•°æ®
        role_semantic_data = {role: data['semantic_data'] for role, data in baselines['roles'].items()}
        role_semantic_path = Path(self.config.output_dir) / "role_baselines_semantic.json"
        with open(role_semantic_path, 'w', encoding='utf-8') as f:
            json.dump(role_semantic_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è§’è‰²è¯­ä¹‰åŸºçº¿å·²ä¿å­˜: {role_semantic_path}")
        
        # ä¿å­˜è§’è‰²åŸºçº¿ - å‘é‡æ•°æ®
        role_vector_data = {role: data['vector_data'] for role, data in baselines['roles'].items()}
        role_vector_path = Path(self.config.output_dir) / "role_baselines_vectors.json"
        with open(role_vector_path, 'w', encoding='utf-8') as f:
            json.dump(role_vector_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è§’è‰²å‘é‡åŸºçº¿å·²ä¿å­˜: {role_vector_path}")
        
        # ä¿å­˜è¡Œä¸šåŸºçº¿ - è¯­ä¹‰æ•°æ®
        industry_semantic_data = {industry: data['semantic_data'] for industry, data in baselines['industries'].items()}
        industry_semantic_path = Path(self.config.output_dir) / "industry_baselines_semantic.json"
        with open(industry_semantic_path, 'w', encoding='utf-8') as f:
            json.dump(industry_semantic_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è¡Œä¸šè¯­ä¹‰åŸºçº¿å·²ä¿å­˜: {industry_semantic_path}")
        
        # ä¿å­˜è¡Œä¸šåŸºçº¿ - å‘é‡æ•°æ®
        industry_vector_data = {industry: data['vector_data'] for industry, data in baselines['industries'].items()}
        industry_vector_path = Path(self.config.output_dir) / "industry_baselines_vectors.json"
        with open(industry_vector_path, 'w', encoding='utf-8') as f:
            json.dump(industry_vector_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ è¡Œä¸šå‘é‡åŸºçº¿å·²ä¿å­˜: {industry_vector_path}")
        
    def _count_semantic_uploads(self, upload_results: Dict) -> int:
        """ç»Ÿè®¡è¯­ä¹‰æ•°æ®ä¸Šä¼ æ•°é‡"""
        count = 0
        if upload_results.get('global', {}).get('semantic_upload', {}).get('status') in ['uploaded', 'exists']:
            count += 1
        for result in upload_results.get('roles', {}).values():
            if result.get('semantic_upload', {}).get('status') in ['uploaded', 'exists']:
                count += 1
        for result in upload_results.get('industries', {}).values():
            if result.get('semantic_upload', {}).get('status') in ['uploaded', 'exists']:
                count += 1
        return count

    def _count_vector_uploads(self, upload_results: Dict) -> int:
        """ç»Ÿè®¡å‘é‡æ•°æ®ä¸Šä¼ æ•°é‡"""
        count = 0
        if upload_results.get('global', {}).get('vector_upload', {}).get('status') in ['uploaded', 'exists']:
            count += 1
        for result in upload_results.get('roles', {}).values():
            if result.get('vector_upload', {}).get('status') in ['uploaded', 'exists']:
                count += 1
        for result in upload_results.get('industries', {}).values():
            if result.get('vector_upload', {}).get('status') in ['uploaded', 'exists']:
                count += 1
        return count

    def _generate_summary_report(self, all_skills: List[Dict], role_classifications: Dict, 
                                industry_classifications: Dict, baselines: Dict, 
                                upload_results: Dict, duration: float) -> Dict:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        # ç»Ÿè®¡åˆ†ç±»ç»“æœ
        total_role_skills = sum(sum(len(skills) for skills in categories.values()) 
                              for categories in role_classifications.values())
        total_industry_skills = sum(sum(len(skills) for skills in categories.values()) 
                                  for categories in industry_classifications.values())
        
        # ç»Ÿè®¡ä¸Šä¼ ç»“æœ - æ›´æ–°ä»¥æ”¯æŒæ–°çš„åˆ†ç¦»ä¸Šä¼ æ ¼å¼
        upload_stats = {
            'global_uploaded': upload_results.get('global', {}).get('status') == 'uploaded',
            'roles_uploaded': sum(1 for result in upload_results.get('roles', {}).values() 
                                if result.get('status') == 'uploaded'),
            'industries_uploaded': sum(1 for result in upload_results.get('industries', {}).values() 
                                     if result.get('status') == 'uploaded'),
            'total_semantic_uploads': self._count_semantic_uploads(upload_results),
            'total_vector_uploads': self._count_vector_uploads(upload_results),
            'total_errors': sum(1 for section in upload_results.values() 
                              for result in (section.values() if isinstance(section, dict) else [section])
                              if result.get('status') == 'error')
        }
        
        return {
            'pipeline_info': {
                'execution_time_seconds': round(duration, 2),
                'completion_time': datetime.now().isoformat(),
                'config': {
                    'model_name': self.config.classification_model,
                    'role_threshold': self.config.role_threshold,
                    'industry_threshold': self.config.industry_threshold,
                    'min_skills_for_baseline': self.config.min_skills_for_baseline
                }
            },
            'skill_processing': {
                'total_skills_extracted': len(all_skills),
                'total_role_assignments': total_role_skills,
                'total_industry_assignments': total_industry_skills,
                'roles_with_skills': len(role_classifications),
                'industries_with_skills': len(industry_classifications)
            },
            'baseline_generation': {
                'global_baseline_generated': 'global' in baselines,
                'role_baselines_generated': len(baselines.get('roles', {})),
                'industry_baselines_generated': len(baselines.get('industries', {})),
                'total_baselines': 1 + len(baselines.get('roles', {})) + len(baselines.get('industries', {}))
            },
            'database_upload': upload_stats,
            'quality_metrics': {
                'classification_coverage': {
                    'role_coverage_percent': round((total_role_skills / len(all_skills)) * 100, 2) if all_skills else 0,
                    'industry_coverage_percent': round((total_industry_skills / len(all_skills)) * 100, 2) if all_skills else 0
                },
                'baseline_quality': {
                    'avg_skills_per_role': round(total_role_skills / len(role_classifications), 2) if role_classifications else 0,
                    'avg_skills_per_industry': round(total_industry_skills / len(industry_classifications), 2) if industry_classifications else 0
                }
            },
            'output_files': {
                'role_classifications': f"{self.config.output_dir}/role_skill_classifications.json",
                'industry_classifications': f"{self.config.output_dir}/industry_skill_classifications.json",
                # æ–°çš„åˆ†ç¦»æ ¼å¼æ–‡ä»¶
                'global_baseline_semantic': f"{self.config.output_dir}/global_baseline_semantic.json",
                'global_baseline_vectors': f"{self.config.output_dir}/global_baseline_vectors.json",
                'role_baselines_semantic': f"{self.config.output_dir}/role_baselines_semantic.json",
                'role_baselines_vectors': f"{self.config.output_dir}/role_baselines_vectors.json",
                'industry_baselines_semantic': f"{self.config.output_dir}/industry_baselines_semantic.json",
                'industry_baselines_vectors': f"{self.config.output_dir}/industry_baselines_vectors.json",
                'summary_report': f"{self.config.output_dir}/pipeline_summary.json"
            }
        }
    
    def _save_summary_report(self, summary: Dict):
        """ä¿å­˜æ€»ç»“æŠ¥å‘Š"""
        summary_path = Path(self.config.output_dir) / "pipeline_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
    
    def _print_summary(self, summary: Dict):
        """æ‰“å°æ€»ç»“ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“Š ç®¡çº¿æ‰§è¡Œæ€»ç»“")
        print("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        pipeline_info = summary['pipeline_info']
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {pipeline_info['execution_time_seconds']} ç§’")
        print(f"ğŸ• å®Œæˆæ—¶é—´: {pipeline_info['completion_time']}")
        
        # æŠ€èƒ½å¤„ç†
        skill_info = summary['skill_processing']
        print(f"\nğŸ“‹ æŠ€èƒ½å¤„ç†:")
        print(f"   æ€»æŠ€èƒ½æ•°: {skill_info['total_skills_extracted']}")
        print(f"   è§’è‰²åˆ†é…: {skill_info['total_role_assignments']}")
        print(f"   è¡Œä¸šåˆ†é…: {skill_info['total_industry_assignments']}")
        print(f"   æœ‰æ•ˆè§’è‰²: {skill_info['roles_with_skills']}")
        print(f"   æœ‰æ•ˆè¡Œä¸š: {skill_info['industries_with_skills']}")
        
        # åŸºçº¿ç”Ÿæˆ
        baseline_info = summary['baseline_generation']
        print(f"\nğŸ—ï¸ åŸºçº¿ç”Ÿæˆ:")
        print(f"   æ€»åŸºçº¿æ•°: {baseline_info['total_baselines']}")
        print(f"   è§’è‰²åŸºçº¿: {baseline_info['role_baselines_generated']}")
        print(f"   è¡Œä¸šåŸºçº¿: {baseline_info['industry_baselines_generated']}")
        
        # æ•°æ®åº“ä¸Šä¼ 
        upload_info = summary['database_upload']
        print(f"\nğŸ“¤ æ•°æ®åº“ä¸Šä¼ :")
        print(f"   å…¨å±€åŸºçº¿: {'âœ…' if upload_info['global_uploaded'] else 'âŒ'}")
        print(f"   è§’è‰²åŸºçº¿: {upload_info['roles_uploaded']} ä¸ª")
        print(f"   è¡Œä¸šåŸºçº¿: {upload_info['industries_uploaded']} ä¸ª")
        print(f"   è¯­ä¹‰æ•°æ®: {upload_info.get('total_semantic_uploads', 0)} ä¸ª")
        print(f"   å‘é‡æ•°æ®: {upload_info.get('total_vector_uploads', 0)} ä¸ª")
        print(f"   é”™è¯¯æ•°é‡: {upload_info['total_errors']}")
        
        # è´¨é‡æŒ‡æ ‡
        quality_info = summary['quality_metrics']
        print(f"\nğŸ“ˆ è´¨é‡æŒ‡æ ‡:")
        print(f"   è§’è‰²è¦†ç›–ç‡: {quality_info['classification_coverage']['role_coverage_percent']}%")
        print(f"   è¡Œä¸šè¦†ç›–ç‡: {quality_info['classification_coverage']['industry_coverage_percent']}%")
        print(f"   å¹³å‡è§’è‰²æŠ€èƒ½: {quality_info['baseline_quality']['avg_skills_per_role']}")
        print(f"   å¹³å‡è¡Œä¸šæŠ€èƒ½: {quality_info['baseline_quality']['avg_skills_per_industry']}")
        
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.config.output_dir}")
        print("="*60)

class EndToEndProcessor:
    """ç«¯åˆ°ç«¯å¤„ç†å™¨ - ä¸»è¦æ¥å£ç±»"""
    
    def __init__(self, config_path: Optional[str] = None, **kwargs):
        """
        åˆå§‹åŒ–ç«¯åˆ°ç«¯å¤„ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            **kwargs: é…ç½®å‚æ•°è¦†ç›–
        """
        self.config = self._load_config(config_path, **kwargs)
        self.orchestrator = PipelineOrchestrator(self.config)
    
    def _load_config(self, config_path: Optional[str], **kwargs) -> PipelineConfig:
        """åŠ è½½é…ç½®"""
        base_config = {}
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½
        if config_path and Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                base_config = json.load(f)
            logger.info(f"âœ… ä»æ–‡ä»¶åŠ è½½é…ç½®: {config_path}")
        
        # åº”ç”¨å‚æ•°è¦†ç›–
        base_config.update(kwargs)
        
        return PipelineConfig(**base_config)
    
    def process_vocabulary(self, vocab_path: str) -> Dict:
        """å¤„ç†æŠ€èƒ½è¯æ±‡è¡¨"""
        skills_vocab = self.orchestrator._load_vocabulary(vocab_path)
        all_skills = self.orchestrator.processor.extract_all_skills(skills_vocab)
        return {
            'vocabulary': skills_vocab,
            'skills': all_skills,
            'total_skills': len(all_skills)
        }
    
    def generate_classifications(self, all_skills: List[Dict]) -> Tuple[Dict, Dict]:
        """ç”ŸæˆæŠ€èƒ½åˆ†ç±»"""
        return self.orchestrator.classifier.classify_skills(all_skills)
    
    def generate_baselines(self, role_classifications: Dict, industry_classifications: Dict) -> Dict:
        """ç”Ÿæˆè¯­ä¹‰åŸºçº¿"""
        return self.orchestrator.baseline_generator.generate_all_baselines(
            role_classifications, industry_classifications
        )
    
    def upload_to_database(self, baselines: Dict) -> Dict:
        """ä¸Šä¼ åŸºçº¿åˆ°æ•°æ®åº“"""
        return self.orchestrator.database_manager.upload_baselines(baselines)
    
    def run_complete_pipeline(self, vocab_path: str) -> Dict:
        """è¿è¡Œå®Œæ•´ç®¡çº¿"""
        return self.orchestrator.run_complete_pipeline(vocab_path)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='JobbAI ç«¯åˆ°ç«¯æŠ€èƒ½å¤„ç†ç®¡çº¿')
    
    # è¾“å…¥è¾“å‡ºå‚æ•°
    parser.add_argument('vocabulary', help='æŠ€èƒ½è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', '-o', default='output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model-name', help='SentenceTransformeræ¨¡å‹åç§°')
    
    # åˆ†çº§é˜ˆå€¼å‚æ•°
    parser.add_argument('--high-confidence', type=float, default=0.75, help='é«˜ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--medium-confidence', type=float, default=0.55, help='ä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼') 
    parser.add_argument('--low-confidence', type=float, default=0.35, help='ä½ç½®ä¿¡åº¦é˜ˆå€¼')
    
    # å…¼å®¹æ—§ç‰ˆæœ¬å‚æ•°
    parser.add_argument('--role-threshold', type=float, help='è§’è‰²åˆ†é…é˜ˆå€¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰')
    parser.add_argument('--industry-threshold', type=float, help='è¡Œä¸šåˆ†é…é˜ˆå€¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰')
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument('--enable-debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ—¥å¿—')
    parser.add_argument('--save-details', action='store_true', default=True, help='ä¿å­˜åˆ†ç±»è¯¦æƒ…')
    
    # æ•°æ®åº“å‚æ•°
    parser.add_argument('--supabase-url', help='Supabase URL')
    parser.add_argument('--supabase-key', help='Supabase API Key')
    parser.add_argument('--skip-database', action='store_true', help='è·³è¿‡æ•°æ®åº“ä¸Šä¼ ')
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='æ—¥å¿—çº§åˆ«')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # å‡†å¤‡é…ç½®å‚æ•°
    config_kwargs = {
        'output_dir': args.output_dir
    }
    
    if args.model_name:
        config_kwargs['model_name'] = args.model_name
    if args.high_confidence is not None:
        config_kwargs['high_confidence_threshold'] = args.high_confidence
    if args.medium_confidence is not None:
        config_kwargs['medium_confidence_threshold'] = args.medium_confidence  
    if args.low_confidence is not None:
        config_kwargs['low_confidence_threshold'] = args.low_confidence
        
    # å…¼å®¹æ—§ç‰ˆæœ¬å‚æ•°
    if args.role_threshold is not None:
        config_kwargs['role_threshold'] = args.role_threshold
    if args.industry_threshold is not None:
        config_kwargs['industry_threshold'] = args.industry_threshold
        
    if args.supabase_url:
        config_kwargs['supabase_url'] = args.supabase_url
    if args.supabase_key:
        config_kwargs['supabase_key'] = args.supabase_key
    if args.skip_database:
        config_kwargs['supabase_url'] = None
        config_kwargs['supabase_key'] = None
    if args.enable_debug:
        config_kwargs['enable_debug_logging'] = True
    if args.save_details is not None:
        config_kwargs['save_classification_details'] = args.save_details
    
    print("ğŸš€ JobbAI ç«¯åˆ°ç«¯æŠ€èƒ½å¤„ç†ç®¡çº¿")
    print(f"ğŸ“ è¯æ±‡è¡¨æ–‡ä»¶: {args.vocabulary}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ”§ é…ç½®æ–‡ä»¶: {args.config or 'ä½¿ç”¨é»˜è®¤é…ç½®'}")
    print("-" * 60)
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = EndToEndProcessor(args.config, **config_kwargs)
        
        # è¿è¡Œå®Œæ•´ç®¡çº¿
        results = processor.run_complete_pipeline(args.vocabulary)
        
        print("\nğŸ‰ ç®¡çº¿æ‰§è¡ŒæˆåŠŸå®Œæˆ!")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ ç®¡çº¿æ‰§è¡Œå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
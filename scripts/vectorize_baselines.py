# import os
# import sys
# import logging
# from dotenv import load_dotenv
# import torch
# import json

# # Add project root to path to allow importing from other directories
# sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# from supabase import create_client, Client
# from sentence_transformers import SentenceTransformer

# # --- Setup ---
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# load_dotenv()

# # --- Configuration - ä½¿ç”¨ä¸ skill_pipeline.py ç›¸åŒçš„æ¨¡å‹é…ç½® ---
# PRIMARY_MODEL_NAME = 'all-MiniLM-L6-v2'
# BACKUP_MODEL_NAME = 'all-mpnet-base-v2'

# # --- Database Connection ---
# SUPABASE_URL = os.getenv("SUPABASE_URL")
# SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# if not SUPABASE_URL or not SUPABASE_KEY:
#     logging.error("Supabase URL and Key must be set in .env file.")
#     sys.exit(1)

# try:
#     supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
#     logging.info("âœ… Successfully connected to Supabase.")
# except Exception as e:
#     logging.error(f"âŒ Failed to connect to Supabase: {e}")
#     sys.exit(1)

# # --- Model Loading - ä¸ skill_pipeline.py ä¿æŒä¸€è‡´çš„åŠ è½½é€»è¾‘ ---
# def load_model():
#     """åŠ è½½ä¸ skill_pipeline.py ç›¸åŒçš„æ¨¡å‹"""
#     logging.info(f"ğŸ¤– å°è¯•åŠ è½½ä¸»æ¨¡å‹: {PRIMARY_MODEL_NAME}...")
#     try:
#         model = SentenceTransformer(PRIMARY_MODEL_NAME)
#         logging.info(f"âœ… æˆåŠŸåŠ è½½ä¸“ç”¨æ¨¡å‹: {PRIMARY_MODEL_NAME}")
#         return model, PRIMARY_MODEL_NAME
#     except Exception as e:
#         logging.warning(f"âš ï¸ æ— æ³•åŠ è½½ä¸“ç”¨æ¨¡å‹ï¼Œä½¿ç”¨å¤‡é€‰æ¨¡å‹: {e}")
#         try:
#             model = SentenceTransformer(BACKUP_MODEL_NAME)
#             logging.info(f"âœ… æˆåŠŸåŠ è½½å¤‡é€‰æ¨¡å‹: {BACKUP_MODEL_NAME}")
#             return model, BACKUP_MODEL_NAME
#         except Exception as e2:
#             logging.error(f"âŒ æ— æ³•åŠ è½½ä»»ä½•æ¨¡å‹: {e2}")
#             sys.exit(1)

# model, actual_model_name = load_model()

# def vectorize_and_store_baselines():
#     """
#     Fetches all semantic baselines, generates vector embeddings for their
#     keywords using the SAME model as skill_pipeline.py, and upserts them 
#     into the baseline_vectors table.
#     """
#     logging.info("Fetching all semantic baselines from the database...")
    
#     # 1. Fetch all baselines (not just active ones)
#     response = supabase.table('semantic_baselines').select('id, name, baseline_data').execute()
    
#     if not response.data:
#         logging.warning("No semantic baselines found to process.")
#         return

#     baselines = response.data
#     logging.info(f"Found {len(baselines)} baselines to process.")

#     processed_count = 0
#     for baseline in baselines:
#         baseline_id = baseline['id']
#         baseline_name = baseline['name']
#         baseline_data = baseline.get('baseline_data', {})

#         # Updated logic to handle nested keyword structure
#         keywords = []
#         if isinstance(baseline_data, dict):
#             # å¤„ç†ä¸‰ç§å¯èƒ½çš„æ•°æ®ç»“æ„
#             if 'categories' in baseline_data:
#                 # è§’è‰²/è¡Œä¸šåŸºçº¿ç»“æ„ï¼šæœ‰ categories å­—æ®µ
#                 categories = baseline_data['categories']
#                 for category_keywords in categories.values():
#                     if isinstance(category_keywords, list):
#                         keywords.extend(category_keywords)
#             elif any(isinstance(v, dict) for v in baseline_data.values()):
#                 # åµŒå¥—å­—å…¸ç»“æ„
#                 for category_data in baseline_data.values():
#                     if isinstance(category_data, dict):
#                         for category_keywords in category_data.values():
#                             if isinstance(category_keywords, list):
#                                 keywords.extend(category_keywords)
#             else:
#                 # ç®€å•å­—å…¸ç»“æ„
#                 for category_keywords in baseline_data.values():
#                     if isinstance(category_keywords, list):
#                         keywords.extend(category_keywords)
        
#         if not keywords:
#             logging.warning(f"Skipping baseline '{baseline_name}' (ID: {baseline_id}) as it has no keywords.")
#             continue

#         # ç¡®ä¿æ‰€æœ‰å…³é”®è¯éƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œè¿‡æ»¤Noneï¼Œç„¶åå»é‡å¹¶æ’åº
#         keywords = sorted(list(set(str(k) for k in keywords if k is not None)))
#         logging.info(f"Processing baseline '{baseline_name}' (ID: {baseline_id}) with {len(keywords)} unique keywords...")

#         try:
#             # 2. Generate embeddings using the same model as skill_pipeline.py
#             embeddings = model.encode(keywords)
            
#             # 3. Structure for JSON storage
#             vectors_data = {keyword: vector.tolist() for keyword, vector in zip(keywords, embeddings)}

#             # 4. Prepare data for upsert - ä½¿ç”¨å®é™…åŠ è½½çš„æ¨¡å‹åç§°
#             data_to_upsert = {
#                 'baseline_id': baseline_id,
#                 'embedding_model': actual_model_name,  # ä½¿ç”¨å®é™…åŠ è½½çš„æ¨¡å‹åç§°
#                 'vectors_data': json.dumps(vectors_data)
#             }

#             # 5. Upsert into the database
#             supabase.table('baseline_vectors').upsert(
#                 data_to_upsert
#             ).execute()

#             processed_count += 1
#             logging.info(f"âœ… Successfully vectorized and stored baseline '{baseline_name}' using {actual_model_name}.")

#         except Exception as e:
#             logging.error(f"âŒ Failed to process baseline '{baseline_name}' (ID: {baseline_id}): {e}", exc_info=True)

#     logging.info(f"\n--- Process Complete ---")
#     logging.info(f"Successfully processed and stored vectors for {processed_count}/{len(baselines)} baselines.")
#     logging.info(f"All vectors generated using model: {actual_model_name}")

# def check_model_consistency():
#     """æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨ä½¿ç”¨ä¸åŒæ¨¡å‹çš„å‘é‡"""
#     logging.info("ğŸ” æ£€æŸ¥æ¨¡å‹ä¸€è‡´æ€§...")
    
#     try:
#         response = supabase.table('baseline_vectors').select('embedding_model').execute()
        
#         if response.data:
#             existing_models = set(row['embedding_model'] for row in response.data)
#             logging.info(f"æ•°æ®åº“ä¸­ç°æœ‰çš„æ¨¡å‹: {existing_models}")
            
#             if len(existing_models) > 1:
#                 logging.warning(f"âš ï¸ æ£€æµ‹åˆ°å¤šä¸ªä¸åŒçš„æ¨¡å‹ï¼è¿™å¯èƒ½å¯¼è‡´å‘é‡ä¸å…¼å®¹ï¼š{existing_models}")
#                 logging.warning("å»ºè®®åˆ é™¤æ—§çš„å‘é‡æ•°æ®æˆ–ç¡®ä¿æ‰€æœ‰ç³»ç»Ÿä½¿ç”¨ç›¸åŒæ¨¡å‹")
#             elif existing_models and actual_model_name not in existing_models:
#                 logging.warning(f"âš ï¸ å½“å‰æ¨¡å‹ {actual_model_name} ä¸æ•°æ®åº“ä¸­çš„æ¨¡å‹ä¸åŒ¹é…ï¼š{existing_models}")
#                 logging.warning("è¿™å°†å¯¼è‡´å‘é‡ç©ºé—´ä¸å…¼å®¹ï¼")
#             else:
#                 logging.info("âœ… æ¨¡å‹ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
#         else:
#             logging.info("æ•°æ®åº“ä¸­æš‚æ— å‘é‡æ•°æ®")
#     except Exception as e:
#         logging.error(f"æ¨¡å‹ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")

# def main():
#     print("JobbAI Baseline Vectorization Tool")
#     print("=" * 50)
#     print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {actual_model_name}")
#     print("æ­¤è„šæœ¬å°†ä½¿ç”¨ä¸ skill_pipeline.py ç›¸åŒçš„æ¨¡å‹å¯¹è¯­ä¹‰åŸºçº¿è¿›è¡Œå‘é‡åŒ–")
#     print("å¹¶å°†ç»“æœå­˜å‚¨åˆ° 'baseline_vectors' è¡¨ä¸­")
    
#     # æ£€æŸ¥æ¨¡å‹ä¸€è‡´æ€§
#     check_model_consistency()
    
#     print(f"\nâš ï¸  é‡è¦è¯´æ˜:")
#     print("ä¸ºäº†ç¡®ä¿å‘é‡å…¼å®¹æ€§ï¼Œè¯·ç¡®ä¿:")
#     print("1. skill_pipeline.py å’Œæ­¤è„šæœ¬ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹")
#     print("2. å¦‚æœæ›´æ¢æ¨¡å‹ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆæ‰€æœ‰å‘é‡")
#     print("3. å»ºè®®åœ¨æ•°æ®åº“ä¸­æ·»åŠ å”¯ä¸€çº¦æŸ:")
#     print("   ALTER TABLE public.baseline_vectors ADD CONSTRAINT unique_baseline_model UNIQUE (baseline_id, embedding_model);")
    
#     user_input = input(f"\nç»§ç»­ä½¿ç”¨ {actual_model_name} è¿›è¡Œå‘é‡åŒ–? (y/n): ")
#     if user_input.lower() == 'y':
#         vectorize_and_store_baselines()
#     else:
#         print("æ“ä½œå·²å–æ¶ˆã€‚")

# if __name__ == "__main__":
#     main()
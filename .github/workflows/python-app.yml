name: Job Analyzer Service

on:
  schedule:
    - cron: '0 3 * * *'  # 每天UTC 3点
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of jobs to process'
        required: false
        default: '50'
  push:
    branches: [ "main" ]
    paths:
      - "analyzer/**"
      - "main.py"
      - "requirements.txt"
      - ".github/workflows/**"

permissions:
  contents: read

jobs:
  analyze:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # 设置超时
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: 'pip'
    
    # 缓存spaCy模型，避免每次下载
    - name: Cache spaCy models
      uses: actions/cache@v3
      with:
        path: ~/.cache/spacy
        key: spacy-models-${{ runner.os }}-v1
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        python -m spacy download de_core_news_sm en_core_web_sm
    
    - name: Run job analyzer
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        TZ: 'Europe/Berlin'
      run: |
        batch_size=${{ github.event.inputs.batch_size || '50' }}
        python main.py --mode inference --supabase --batch-size $batch_size
    
    # 添加结果报告
    - name: Generate summary
      if: always()
      run: |
        echo "Job analysis completed at $(date)"
        echo "Check Supabase for results"